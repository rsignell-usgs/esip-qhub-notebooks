{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "italian-invitation",
   "metadata": {},
   "source": [
    "# Calculation wave climate from COAWST  forecast\n",
    "\n",
    "We rechunk the original netcdf data to time series orientation in Zarr using rechunker, then use xarray map_blocks to process each block along the time dimension. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.distributed\n",
    "from dask.distributed import Client, performance_report\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "from rechunker import rechunk\n",
    "import fsspec\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zarr\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctype = 'standard'\n",
    "#ctype = 'usgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ctype == 'usgs':\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    cluster = SLURMCluster(cores=36, memory='128GiB', \n",
    "                    interface='ib0', \n",
    "                    local_directory='$SCRATCH',\n",
    "                    job_extra=['--qos usgs', '--partition usgs'], walltime='03:00:00')\n",
    "    print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ctype == 'standard':\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    cluster = SLURMCluster(cores=36, memory='128GiB', \n",
    "                project='science', interface='ib0', \n",
    "                local_directory='$SCRATCH',\n",
    "                queue='compute', walltime='03:00:00')\n",
    "    print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs.glob('/vortexfs1/share/usgs-share/Projects/COAWST/2020/coawst_us_*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fgood = fs.glob('/vortexfs1/share/usgs-share/Projects/COAWST/20*/coawst_us_20*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fgood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgood = np.sort(fgood)\n",
    "print(fgood[0])\n",
    "print(fgood[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechunker_wrapper(source_store, target_store, temp_store, chunks=None, mem=\"2GiB\", consolidated=True, verbose=True):\n",
    "\n",
    "    # convert str to paths\n",
    "    def maybe_convert_to_path(p):\n",
    "        if isinstance(p, str):\n",
    "            return Path(p)\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    source_store = maybe_convert_to_path(source_store)\n",
    "    target_store = maybe_convert_to_path(target_store)\n",
    "    temp_store = maybe_convert_to_path(temp_store)\n",
    "\n",
    "    # erase target and temp stores\n",
    "    if temp_store.exists():\n",
    "        shutil.rmtree(temp_store)\n",
    "\n",
    "    if target_store.exists():\n",
    "        shutil.rmtree(target_store)\n",
    "\n",
    "\n",
    "    if isinstance(source_store, xr.Dataset):\n",
    "        g = source_store  # trying to work directly with a dataset\n",
    "        ds_chunk = g\n",
    "    else:\n",
    "        g = zarr.group(str(source_store))\n",
    "        # get the correct shape from loading the store as xr.dataset and parse the chunks\n",
    "        ds_chunk = xr.open_zarr(str(source_store))\n",
    "        \n",
    "\n",
    "    # convert all paths to strings\n",
    "    source_store = str(source_store)\n",
    "    target_store = str(target_store)\n",
    "    temp_store = str(temp_store)\n",
    "\n",
    "    group_chunks = {}\n",
    "    # newer tuple version that also takes into account when specified chunks are larger than the array\n",
    "    for var in ds_chunk.variables:\n",
    "        # pick appropriate chunks from above, and default to full length chunks for dimensions that are not in `chunks` above.\n",
    "        group_chunks[var] = []\n",
    "        for di in ds_chunk[var].dims:\n",
    "            if di in chunks.keys():\n",
    "                if chunks[di] > len(ds_chunk[di]):\n",
    "                    group_chunks[var].append(len(ds_chunk[di]))\n",
    "                else:\n",
    "                    group_chunks[var].append(chunks[di])\n",
    "\n",
    "            else:\n",
    "                group_chunks[var].append(len(ds_chunk[di]))\n",
    "\n",
    "        group_chunks[var] = tuple(group_chunks[var])\n",
    "    if verbose:\n",
    "        print(f\"Rechunking to: {group_chunks}\")\n",
    "    rechunked = rechunk(g, group_chunks, mem, target_store, temp_store=temp_store)\n",
    "    rechunked.execute(retries=10)\n",
    "    if consolidated:\n",
    "        if verbose:\n",
    "            print('consolidating metadata')\n",
    "        zarr.convenience.consolidate_metadata(target_store)\n",
    "    if verbose:\n",
    "        print('removing temp store')\n",
    "    shutil.rmtree(temp_store)\n",
    "    if verbose:\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(ds):\n",
    "    drop_list=[]\n",
    "    for var in ds.variables:\n",
    "        if len(ds[var].dims) == 0:\n",
    "            drop_list.append(ds[var].name) \n",
    "        if var in ['wetdry_mask_psi','wetdry_mask_rho','wetdry_mask_u','wetdry_mask_v']:\n",
    "            drop_list.append(ds[var].name)\n",
    "\n",
    "    return ds.drop(drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = '/vortexfs1/scratch/aretxabaleta/rechunk/zarr'\n",
    "#prefix = '/vortexfs1/usgs/rsignell/alfredo/rechunk/zarr'\n",
    "prefix = '/vortexfs1/scratch/rsignell/alfredo/rechunk/zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_step = f'{prefix}/step'\n",
    "zarr_temp = f'{prefix}/tmp'\n",
    "zarr_chunked = f'{prefix}/coawst_fc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mem = '16GB'\n",
    "time_chunk_size = 144\n",
    "time_steps_per_file = 12\n",
    "# x_chunk_size = 300\n",
    "# y_chunk_size = 300\n",
    "x_chunk_size = 100\n",
    "y_chunk_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_chunks = int(np.ceil(len(fgood)*time_steps_per_file/time_chunk_size))\n",
    "nt_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "First=True\n",
    "try:\n",
    "    shutil.rmtree(zarr_chunked, ignore_errors=False, onerror=None)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(1):\n",
    "#for i in range(nt_chunks):\n",
    "#for i in range(9,nt_chunks):\n",
    "#for i in range(nt_chunks-3, nt_chunks):\n",
    "    start = time.time()\n",
    "    print(i)\n",
    "    istart = i * int(time_chunk_size/time_steps_per_file)\n",
    "    istop = int(np.min([(i+1) * int(time_chunk_size/time_steps_per_file), len(fgood)]))\n",
    "    \n",
    "    ds = xr.open_mfdataset(fgood[istart:istop], chunks={'ocean_time':time_steps_per_file}, decode_timedelta=False,\n",
    "                           data_vars=\"minimal\", coords=\"minimal\", compat=\"override\", parallel=True,\n",
    "                           preprocess=drop)\n",
    "    \n",
    "    rechunker_wrapper(ds, zarr_step, zarr_temp, mem=max_mem, consolidated=True, verbose=False,\n",
    "                  chunks={'s_rho':1, 's_w':1, 'ocean_time':144, 'ND':1, 'Nbed':1,\n",
    "                         'eta_rho':100, 'xi_rho':100, 'eta_u':100, 'xi_u':100,\n",
    "                         'eta_v':100, 'xi_v':100})\n",
    "    \n",
    "\n",
    "    # read back in the zarr chunk rechunker wrote\n",
    "    ds = xr.open_zarr(zarr_step, consolidated=True)\n",
    "\n",
    "    if First is True:\n",
    "        ds.to_zarr(zarr_chunked, consolidated=True, mode='w')\n",
    "        First = False\n",
    "    else:\n",
    "    #    ds.to_zarr(zarr_chunked, consolidated=True, append_dim='time')\n",
    "        ds.to_zarr(zarr_chunked, consolidated=True, append_dim='ocean_time')\n",
    "    print(f'completed in {start - time.time()} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-salmon",
   "metadata": {},
   "source": [
    "#### Check resulting dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_woopwoop = xr.open_zarr(zarr_chunked, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_woopwoop.temp[:,-1,180,500].hvplot(x='ocean_time', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-delight",
   "metadata": {},
   "source": [
    "#### Some extra cells below to explore if something goes wrong with a particular zarr chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(fgood[istart:istart+6], chunks={'ocean_time':time_steps_per_file}, decode_timedelta=False,\n",
    "                           data_vars=\"minimal\", coords=\"minimal\", compat=\"override\", parallel=True,\n",
    "                           preprocess=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgood[istart:istop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dims['ocean_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=9\n",
    "istart = i * int(time_chunk_size/time_steps_per_file)\n",
    "istop = int(np.min([(i+1) * int(time_chunk_size/time_steps_per_file), len(fgood)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for f in fgood[istart:istop]:\n",
    "    ds = xr.open_dataset(f)\n",
    "    print(f)\n",
    "    print(ds.ocean_time.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.rename('/vortexfs1/share/usgs-share/Projects/COAWST/2012/coawst_us_20120902_13.nc',\n",
    "          '/vortexfs1/share/usgs-share/Projects/COAWST/bad_files/coawst_us_20120902_13.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-context",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
