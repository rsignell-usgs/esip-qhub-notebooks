{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import zarr\n",
    "from zarr.meta import encode_fill_value\n",
    "from zarr.storage import _path_to_prefix\n",
    "from numcodecs import Zlib\n",
    "import fsspec\n",
    "\n",
    "lggr = logging.getLogger('h5-to-zarr')\n",
    "chunks_meta_key = \".zchunkstore\"\n",
    "\n",
    "\n",
    "class Hdf5ToZarr:\n",
    "    \"\"\"Translate the content of one HDF5 file into Zarr metadata.\n",
    "\n",
    "    HDF5 groups become Zarr groups. HDF5 datasets become Zarr arrays. Zarr array\n",
    "    chunks remain in the HDF5 file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h5f : file-like or str\n",
    "        Input HDF5 file as a string or file-like Python object.\n",
    "    store : MutableMapping\n",
    "        Zarr store.\n",
    "    xarray : bool, optional\n",
    "        Produce attributes required by the `xarray <http://xarray.pydata.org>`_\n",
    "        package to correctly identify dimensions (HDF5 dimension scales) of a\n",
    "        Zarr array. Default is ``False``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h5f, url, xarray=False):\n",
    "        # Open HDF5 file in read mode...\n",
    "        lggr.debug(f'HDF5 file: {h5f}')\n",
    "        lggr.debug(f'xarray: {xarray}')\n",
    "        self._h5f = h5py.File(h5f, mode='r')\n",
    "        self._xr = xarray\n",
    "\n",
    "        self.store = {}\n",
    "        self._zroot = zarr.group(store=self.store, overwrite=True)\n",
    "\n",
    "        self._uri = url\n",
    "        lggr.debug(f'Source URI: {self._uri}')\n",
    "\n",
    "    def translate(self):\n",
    "        \"\"\"Translate content of one HDF5 file into Zarr storage format.\n",
    "\n",
    "        No data is copied out of the HDF5 file.\n",
    "        \"\"\"\n",
    "        import json\n",
    "        lggr.debug('Translation begins')\n",
    "        self.transfer_attrs(self._h5f, self._zroot)\n",
    "        self._h5f.visititems(self.translator)\n",
    "        ref = {}\n",
    "        for key, value in self.store.items():\n",
    "            if key.endswith(\".zchunkstore\"):\n",
    "                value = json.loads(value)\n",
    "                source = value.pop(\"source\")[\"uri\"]\n",
    "                for k, v in value.items():\n",
    "                    ref[k] = (source, v[\"offset\"], v[\"size\"])\n",
    "            else:\n",
    "                ref[key] = value.decode()\n",
    "        return ref\n",
    "\n",
    "    def transfer_attrs(self, h5obj, zobj):\n",
    "        \"\"\"Transfer attributes from an HDF5 object to its equivalent Zarr object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h5obj : h5py.Group or h5py.Dataset\n",
    "            An HDF5 group or dataset.\n",
    "        zobj : zarr.hierarchy.Group or zarr.core.Array\n",
    "            An equivalent Zarr group or array to the HDF5 group or dataset with\n",
    "            attributes.\n",
    "        \"\"\"\n",
    "        for n, v in h5obj.attrs.items():\n",
    "            if n in ('REFERENCE_LIST', 'DIMENSION_LIST'):\n",
    "                continue\n",
    "\n",
    "            # Fix some attribute values to avoid JSON encoding exceptions...\n",
    "            if isinstance(v, bytes):\n",
    "                v = v.decode('utf-8')\n",
    "            elif isinstance(v, (np.ndarray, np.number)):\n",
    "                if n == '_FillValue':\n",
    "                    v = encode_fill_value(v, v.dtype)\n",
    "                elif v.size == 1:\n",
    "                    v = v.flatten()[0].tolist()\n",
    "                else:\n",
    "                    v = v.tolist()\n",
    "            if self._xr and v == 'DIMENSION_SCALE':\n",
    "                continue\n",
    "            try:\n",
    "                zobj.attrs[n] = v\n",
    "            except TypeError:\n",
    "                print(f'Caught TypeError: {n}@{h5obj.name} = {v} ({type(v)})')\n",
    "\n",
    "    def translator(self, name, h5obj):\n",
    "        \"\"\"Produce Zarr metadata for all groups and datasets in the HDF5 file.\n",
    "        \"\"\"\n",
    "        if isinstance(h5obj, h5py.Dataset):\n",
    "            lggr.debug(f'Dataset: {h5obj.name}')\n",
    "            if (h5obj.scaleoffset or h5obj.fletcher32 or h5obj.shuffle or\n",
    "                    h5obj.compression in ('szip', 'lzf')):\n",
    "                raise RuntimeError(\n",
    "                    f'{h5obj.name} uses unsupported HDF5 filters')\n",
    "            if h5obj.compression == 'gzip':\n",
    "                compression = Zlib(level=h5obj.compression_opts)\n",
    "            else:\n",
    "                compression = None\n",
    "\n",
    "            # Get storage info of this HDF5 dataset...\n",
    "            cinfo = self.storage_info(h5obj)\n",
    "            if self._xr and h5py.h5ds.is_scale(h5obj.id) and not cinfo:\n",
    "                return\n",
    "\n",
    "            # Create a Zarr array equivalent to this HDF5 dataset...\n",
    "            za = self._zroot.create_dataset(h5obj.name, shape=h5obj.shape,\n",
    "                                            dtype=h5obj.dtype,\n",
    "                                            chunks=h5obj.chunks or False,\n",
    "                                            fill_value=h5obj.fillvalue,\n",
    "                                            compression=compression,\n",
    "                                            overwrite=True)\n",
    "            lggr.debug(f'Created Zarr array: {za}')\n",
    "            self.transfer_attrs(h5obj, za)\n",
    "\n",
    "            if self._xr:\n",
    "                # Do this for xarray...\n",
    "                adims = self._get_array_dims(h5obj)\n",
    "                za.attrs['_ARRAY_DIMENSIONS'] = adims\n",
    "                lggr.debug(f'_ARRAY_DIMENSIONS = {adims}')\n",
    "\n",
    "            # Store chunk location metadata...\n",
    "            if cinfo:\n",
    "                cinfo['source'] = {'uri': self._uri,\n",
    "                                   'array_name': h5obj.name}\n",
    "                chunks_info(za, cinfo)\n",
    "\n",
    "        elif isinstance(h5obj, h5py.Group):\n",
    "            lggr.debug(f'Group: {h5obj.name}')\n",
    "            zgrp = self._zroot.create_group(h5obj.name)\n",
    "            self.transfer_attrs(h5obj, zgrp)\n",
    "\n",
    "    def _get_array_dims(self, dset):\n",
    "        \"\"\"Get a list of dimension scale names attached to input HDF5 dataset.\n",
    "\n",
    "        This is required by the xarray package to work with Zarr arrays. Only\n",
    "        one dimension scale per dataset dimension is allowed. If dataset is\n",
    "        dimension scale, it will be considered as the dimension to itself.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dset : h5py.Dataset\n",
    "            HDF5 dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List with HDF5 path names of dimension scales attached to input\n",
    "            dataset.\n",
    "        \"\"\"\n",
    "        dims = list()\n",
    "        rank = len(dset.shape)\n",
    "        if rank:\n",
    "            for n in range(rank):\n",
    "                num_scales = len(dset.dims[n])\n",
    "                if num_scales == 1:\n",
    "                    dims.append(dset.dims[n][0].name[1:])\n",
    "                elif h5py.h5ds.is_scale(dset.id):\n",
    "                    dims.append(dset.name[1:])\n",
    "                elif num_scales > 1:\n",
    "                    raise RuntimeError(\n",
    "                        f'{dset.name}: {len(dset.dims[n])} '\n",
    "                        f'dimension scales attached to dimension #{n}')\n",
    "        return dims\n",
    "\n",
    "    def storage_info(self, dset):\n",
    "        \"\"\"Get storage information of an HDF5 dataset in the HDF5 file.\n",
    "\n",
    "        Storage information consists of file offset and size (length) for every\n",
    "        chunk of the HDF5 dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dset : h5py.Dataset\n",
    "            HDF5 dataset for which to collect storage information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            HDF5 dataset storage information. Dict keys are chunk array offsets\n",
    "            as tuples. Dict values are pairs with chunk file offset and size\n",
    "            integers.\n",
    "        \"\"\"\n",
    "        # Empty (null) dataset...\n",
    "        if dset.shape is None:\n",
    "            return dict()\n",
    "\n",
    "        dsid = dset.id\n",
    "        if dset.chunks is None:\n",
    "            # Contiguous dataset...\n",
    "            if dsid.get_offset() is None:\n",
    "                # No data ever written...\n",
    "                return dict()\n",
    "            else:\n",
    "                key = (0,) * (len(dset.shape) or 1)\n",
    "                return {key: {'offset': dsid.get_offset(),\n",
    "                              'size': dsid.get_storage_size()}}\n",
    "        else:\n",
    "            # Chunked dataset...\n",
    "            num_chunks = dsid.get_num_chunks()\n",
    "            if num_chunks == 0:\n",
    "                # No data ever written...\n",
    "                return dict()\n",
    "\n",
    "            # Go over all the dataset chunks...\n",
    "            stinfo = dict()\n",
    "            chunk_size = dset.chunks\n",
    "            for index in range(num_chunks):\n",
    "                blob = dsid.get_chunk_info(index)\n",
    "                key = tuple(\n",
    "                    [a // b for a, b in zip(blob.chunk_offset, chunk_size)])\n",
    "                stinfo[key] = {'offset': blob.byte_offset,\n",
    "                               'size': blob.size}\n",
    "            return stinfo\n",
    "\n",
    "\n",
    "def chunks_info(zarray, chunks_loc):\n",
    "    \"\"\"Store chunks location information for a Zarr array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zarray : zarr.core.Array\n",
    "        Zarr array that will use the chunk data.\n",
    "    chunks_loc : dict\n",
    "        File storage information for the chunks belonging to the Zarr array.\n",
    "    \"\"\"\n",
    "    if 'source' not in chunks_loc:\n",
    "        raise ValueError('Chunk source information missing')\n",
    "    if any([k not in chunks_loc['source'] for k in ('uri', 'array_name')]):\n",
    "        raise ValueError(\n",
    "            f'{chunks_loc[\"source\"]}: Chunk source information incomplete')\n",
    "\n",
    "    key = _path_to_prefix(zarray.path) + chunks_meta_key\n",
    "    chunks_meta = dict()\n",
    "    for k, v in chunks_loc.items():\n",
    "        if k != 'source':\n",
    "            k = zarray._chunk_key(k)\n",
    "            if any([a not in v for a in ('offset', 'size')]):\n",
    "                raise ValueError(\n",
    "                    f'{k}: Incomplete chunk location information')\n",
    "        chunks_meta[k] = v\n",
    "\n",
    "    # Store Zarr array chunk location metadata...\n",
    "    zarray.store[key] = json.dumps(chunks_meta)\n",
    "\n",
    "\n",
    "def run(url, **storage_options):\n",
    "    lggr.setLevel(logging.DEBUG)\n",
    "    lggr_handler = logging.StreamHandler()\n",
    "    lggr_handler.setFormatter(logging.Formatter(\n",
    "        '%(levelname)s:%(name)s:%(funcName)s:%(message)s')\n",
    "    )\n",
    "    lggr.handlers.clear()\n",
    "    lggr.addHandler(lggr_handler)\n",
    "\n",
    "    with fsspec.open(url, **storage_options) as f:\n",
    "        h5chunks = Hdf5ToZarr(f, url, xarray=True)\n",
    "        return h5chunks.translate()\n",
    "\n",
    "\n",
    "def example():\n",
    "    return run(\n",
    "        's3://pangeo-data-uswest2/esip/adcirc/adcirc_01d.nc',\n",
    "        mode='rb', anon=False, requester_pays=True,\n",
    "        default_fill_cache=False, default_cache_type='none'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('adcirc_01d_offsets.json', 'w') as fp:\n",
    "    json.dump(metadata_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
