{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Land Use / Land Cover\n",
    "\n",
    "This notebook demonstrates applying a land use classification model to NAIP data.\n",
    "\n",
    "It does a batch prediction on many scenese that have been merged together, using a cluster of machines, each with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from distributed import Client, wait\n",
    "import adlfs\n",
    "import utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "import dask\n",
    "import itertools\n",
    "from dask_gateway import Gateway\n",
    "import rasterio\n",
    "import affine\n",
    "import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Setup\n",
    "\n",
    "We'll make a cluster of Dask workers running on AKS (using an autoscaling VM scaleset) where each worker has a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_WORKERS = 4\n",
    "# g = Gateway()\n",
    "# options = g.cluster_options()\n",
    "# options['gpu'] = True\n",
    "# options['worker_memory'] = 64  # TODO: bump to 100\n",
    "# options[\"worker_cores\"] = 5\n",
    "# display(options)\n",
    "\n",
    "# cluster = g.new_cluster(options)\n",
    "# client = cluster.get_client()\n",
    "# cluster.scale(N_WORKERS)\n",
    "# cluster\n",
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster(resources={\"gpu\": 1}, threads_per_worker=5)\n",
    "client = Client(cluster)\n",
    "N_WORKERS = len(client.scheduler_info()['workers'])\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Discovery\n",
    "\n",
    "Using the Planetary Computer's metadata query API, we can select a region of interest and get back the URLs to the assets in Azure Blob Storage. In this case, it's NAIP scenes from 2013 and 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be replaced by the query API. For now we just grab some images.\n",
    "fs = adlfs.AzureBlobFileSystem(account_name=\"naipeuwest\")\n",
    "blobs_2013 = [f\"/vsicurl/{fs.account_url}/{blob}\" for blob in fs.glob(\"naip/v002/md/2013/md_100cm_2013/39076/*.tif\")]\n",
    "blobs_2017 = [f\"/vsicurl/{fs.account_url}/{blob}\" for blob in fs.glob(\"naip/v002/md/2017/md_100cm_2017/39076/*.tif\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mosaic / Alignment\n",
    "\n",
    "We have many files in blob storage. We want to treat them as one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mosaic2013 = gdal.BuildVRT(\"mosaic-2013.vrt\", blobs_2013[:12])\n",
    "mosaic2013.FlushCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mosaic2017 = gdal.BuildVRT(\"mosaic-2017.vrt\", blobs_2017[:12])\n",
    "mosaic2017.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2013 files need to be aligned to the 2013 grid for comparision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rasterio.open(\"mosaic-2013.vrt\")\n",
    "b = rasterio.open(\"mosaic-2017.vrt\")\n",
    "options = gdal.WarpOptions(\n",
    "    outputBounds=tuple(a.bounds),\n",
    "    width=a.width,\n",
    "    height=a.height,\n",
    ")\n",
    "warped2017 = gdal.Warp(\"warped-2017.vrt\", \"mosaic-2017.vrt\", options=options)\n",
    "warped2017.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalize the cluster setup. Gets the VRT to the workers and ensure they're properly registered with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make this workflow nicer.\n",
    "# 1. VRT: We could perhaps pre-generate this and throw it in a bucket somewhere.\n",
    "# 2. Worker resources: https://github.com/dask/distributed/pull/4456\n",
    "\n",
    "# In the future we'll set\n",
    "# options['environment'] = {\"DASK_DISTRIBUTED__WORKER__RESOURCES__GPU\": \"1\"}\n",
    "# \n",
    "\n",
    "import os\n",
    "import logging\n",
    "from distributed.diagnostics.plugin import WorkerPlugin\n",
    "\n",
    "\n",
    "class UploadVRTs(WorkerPlugin):\n",
    "    name = \"upload_vrt\"\n",
    "    def __init__(self, vrt_filenames):\n",
    "        self.vrt_filenames = vrt_filenames\n",
    "        vrt_data = {}\n",
    "        for vrt_filename in vrt_filenames:\n",
    "            with open(vrt_filename, \"rb\") as f:\n",
    "                vrt_data[vrt_filename] = f.read()\n",
    "\n",
    "        self.vrt_data = vrt_data\n",
    "\n",
    "    def setup(self, worker):\n",
    "        logger = logging.getLogger(\"distributed.worker\")\n",
    "        logger.info(\"Copying vrt for %s\", worker)\n",
    "        import subprocess\n",
    "        subprocess.call(['pip', 'install', '-U', 'git+https://github.com/corteva/rioxarray.git'])\n",
    "        for vrt_filename in self.vrt_filenames:\n",
    "            if not os.path.exists(vrt_filename):\n",
    "                with open(vrt_filename, \"wb\") as f:\n",
    "                    f.write(self.vrt_data[vrt_filename])\n",
    "\n",
    "\n",
    "# We're hacking things to set worker resources. Fixed properly in https://github.com/dask/distributed/pull/4456\n",
    "async def set_resources(dask_worker, **resources):\n",
    "    await dask_worker.set_resources(**resources)\n",
    "    return dask_worker.total_resources\n",
    "\n",
    "\n",
    "client.wait_for_workers(N_WORKERS)\n",
    "\n",
    "# vrt\n",
    "plugin = UploadVRTs([\"mosaic-2013.vrt\", \"warped-2017.vrt\"])\n",
    "client.register_worker_plugin(plugin)\n",
    "# gpu\n",
    "resources = {\"gpu\": 1}\n",
    "client.run(set_resources, gpu=1)\n",
    "# utils\n",
    "client.upload_file(\"utils.py\")\n",
    "\n",
    "client.wait_for_workers(N_WORKERS)\n",
    "\n",
    "# model\n",
    "model = utils.load_model(\"/srv/unet_both_lc.pt\")\n",
    "remote_model = client.scatter(model, broadcast=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocessing\n",
    "\n",
    "xarray provides a convinent data structure for working with large, labeled datasets like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = rioxarray.open_rasterio(\"mosaic-2013.vrt\", chunks=(4, 4096, 4096), lock=False)\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = rioxarray.open_rasterio(\"warped-2017.vrt\", chunks=(4, 4096, 4096), lock=False)\n",
    "ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a big dataset on an aligned grid for the two time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.concat([ds1, ds2], dim=\"time\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model requires a bit of preprocessing upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = (ds - utils.mean) / utils.std\n",
    "normalized = normalized.chunk(utils.CHUNKS)\n",
    "\n",
    "# Avoid predictions on partial chunks\n",
    "normalized = normalized.isel(\n",
    "    y=slice(-(normalized.shape[2] % utils.CHIP_SIZE)),\n",
    "    x=slice(-(normalized.shape[3] % utils.CHIP_SIZE))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Predict\n",
    "\n",
    "The only wrinkle here is that our dataset is much larger than our available GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prediction collapses the 4D variable of bands down to a 1D classification.\n",
    "template = normalized.isel(band=0).drop_vars([\"band\", \"spatial_ref\"]).astype(\"uint8\")\n",
    "\n",
    "with dask.annotate(resources={'gpu': 1}):\n",
    "    predictions = normalized.map_blocks(utils.predict_datarray,\n",
    "                                        kwargs=dict(model=remote_model), template=template)\n",
    "    predictions = predictions.rio.set_crs(normalized.rio.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = predictions.sel(time=0) != predictions.sel(time=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there's some per-pixel (or sub-pixel) differences in alignment. So we'll smooth the output. We'll flag a pixel as changed only when it an all of its neighbors have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, stencil\n",
    "import numpy as np\n",
    "\n",
    "@stencil(neighborhood=((-3, 2), (-3, 2)))\n",
    "def _smooth(x):\n",
    "    base = result = x[0, 0]\n",
    "    for i in range(-3, 3):\n",
    "        for j in range(-3, 3):\n",
    "            if x[i, j] != base:\n",
    "                result = 0\n",
    "                break\n",
    "    return result\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def smooth(x):\n",
    "    return _smooth(x)\n",
    "\n",
    "\n",
    "n_classes = utils.lc_cmap.N\n",
    "change2 = xr.where(change, 0, n_classes * predictions.sel(time=0) + predictions.sel(time=1))\n",
    "smoothed = change2.data.map_overlap(smooth, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_, predictions_ = client.persist([ds, predictions], optimize_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = predictions_.sel(time=0) != predictions_.sel(time=1)\n",
    "change2 = xr.where(~change, 0, n_classes * predictions_.sel(time=0) + predictions_.sel(time=1))\n",
    "smoothed = smooth(change2.compute().data) != 0\n",
    "smoothed = xr.DataArray(smoothed, coords=change2.coords, dims=change2.dims, attrs=change2.attrs)\n",
    "changed_predictions = predictions_.astype(\"float32\").where(smoothed).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import panel\n",
    "\n",
    "kwargs = dict(x=\"x\", y=\"y\", cmap=utils.lc_cmap, rasterize=True,\n",
    "              aggregator=\"mode\", clim=(0, utils.lc_cmap.N - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.Column(\n",
    "    panel.Row(\n",
    "        ds_.sel(time=0).hvplot.rgb(bands=\"band\", rasterize=True),\n",
    "        changed_predictions.sel(time=0).hvplot.image(**kwargs),\n",
    "    ),\n",
    "    panel.Row(\n",
    "        ds_.sel(time=1).hvplot.rgb(bands=\"band\", rasterize=True),\n",
    "        changed_predictions.sel(time=1).hvplot.image(**kwargs),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Write to Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.storage.blob\n",
    "import dask.base\n",
    "import dask.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = os.environ[\"AZURE_CONNECTION_STRING\"]\n",
    "CRS = predictions.rio.crs\n",
    "def rename_key(key):\n",
    "    return 'naip/' + str(key).replace(\", \", \"-\").replace(\"'\", \"\").strip(\"()\") + \".tif\"\n",
    "\n",
    "\n",
    "def write_chunk(chunk, year, slice_):\n",
    "    memory_file = rasterio.io.MemoryFile()\n",
    "    chunk = chunk.rio.set_crs(CRS)\n",
    "    chunk.rio.to_raster(memory_file)\n",
    "    memory_file.seek(0)\n",
    "\n",
    "    client = azure.storage.blob.ContainerClient.from_connection_string(\n",
    "        connection_string,\n",
    "        container_name=\"pangeo-scratch\"\n",
    "    )\n",
    "    name = f'naip/{year}/{dask.base.tokenize(slice_)}.tif'\n",
    "    client.upload_blob(name, memory_file, length=len(memory_file), overwrite=True)\n",
    "    return name\n",
    "\n",
    "writes = []\n",
    "for year in [0, 1]:\n",
    "    subset = predictions[year]\n",
    "    slices = dask.array.core.slices_from_chunks(subset.chunks)\n",
    "    writes.extend([dask.delayed(write_chunk)(predictions[(year,) + x], year, x) for x in slices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = client.compute(writes, optimize_graph=False)\n",
    "_ = wait(keys);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
