{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing NWM data\n",
    "Read 200,000+ NetCDF files (each about 40MB) from the National Water Model version 2 and create rechunked Zarr. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import fsspec\n",
    "import dask\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, performance_report\n",
    "import xarray\n",
    "xarray.set_options(display_style=\"text\")   # html repr is 14mb!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_type = 'Fargate'\n",
    "cluster_type = 'Gateway'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_type == 'Fargate':\n",
    "    import boto3\n",
    "    ecs = boto3.client('ecs')\n",
    "    resp = ecs.list_clusters()\n",
    "    clusters = resp['clusterArns']\n",
    "    if len(clusters) > 1:\n",
    "        print(\"Please manually select your cluster\")\n",
    "    cluster = clusters[0]\n",
    "    numWorkers=70\n",
    "    ecs.update_service(cluster=cluster, service='Dask-Worker', desiredCount=numWorkers)\n",
    "    ecs.get_waiter('services_stable').wait(cluster=cluster, services=['Dask-Worker'])\n",
    "    client = Client('Dask-Scheduler.local-dask:8786')\n",
    "\n",
    "elif cluster_type == 'Gateway':\n",
    "    import dask_gateway\n",
    "    gateway = dask_gateway.Gateway()\n",
    "    cluster = gateway.new_cluster(environment='pangeo', profile='Medium Worker')\n",
    "    cluster.adapt(minimum=2, maximum=18)\n",
    "    client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close();cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the list of NWM Streamflow files on AWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nwm_bucket = 's3://noaa-nwm-retro-v2.0-pds'\n",
    "nwm_type = 'CHRTOUT'\n",
    "\n",
    "dates = pd.date_range(start='1993-01-01 00:00',end='2018-12-31 23:00', freq='1h')\n",
    "files = ['{}/full_physics/{}/{}.{}_DOMAIN1.comp'.format(nwm_bucket,date.strftime('%Y'),\n",
    "            date.strftime('%Y%m%d%H%M'),nwm_type) for date in dates]\n",
    "print(files[0])\n",
    "print(files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open an example file and check the native chunking\n",
    "We want to chunk in a similar way for maximum performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = files[0]\n",
    "ncfile = fsspec.open(url)\n",
    "ds0 = xr.open_dataset(ncfile.open())\n",
    "feature_id = ds0.feature_id\n",
    "ds0.streamflow.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0.streamflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open all the data as a single dataset (avoiding open_mfdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def s3open_data(path):\n",
    "    fs = fsspec.filesystem('s3', anon=True, default_fill_cache=False)\n",
    "    f = fs.open(path)\n",
    "    ds = xr.open_dataset(f) \n",
    "    return ds['streamflow'].values\n",
    "\n",
    "files_mapper = [s3open_data(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shape = ds0.streamflow.shape\n",
    "dtype = ds0.streamflow.dtype\n",
    "data_mapper = [dask.array.from_delayed(f, shape, dtype=dtype) for f in files_mapper]\n",
    "all_data = dask.array.stack(data_mapper)\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Dask array to Xarray dataset, copying attributes from sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.DataArray(all_data, coords=[dates, feature_id], dims=[\"time\", \"feature_id\"])\n",
    "\n",
    "ds = da.to_dataset(name='streamflow')\n",
    "ds = ds.assign_attrs(ds0.attrs)\n",
    "ds['time'] = ds['time'].assign_attrs(ds0.time.attrs)\n",
    "ds['feature_id'] = ds['feature_id'].assign_attrs(ds0.feature_id.attrs)\n",
    "ds['streamflow'] = ds['streamflow'].assign_attrs(ds0.streamflow.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.streamflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure we can access the private bucket we wish to write to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs2 = fsspec.filesystem('s3',anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs2.ls('/esip-qhub/noaa/NWM2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try writing 6 days (144 time steps) to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rechunker import rechunk\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds.sel(time=slice('2015-01-01 00:00','2015-01-06 23:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to Zarr (no rechunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    ds_test.to_zarr(fsspec.get_mapper('s3://esip-qhub/noaa/NWM2/test_zarr2'), mode='w', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to Zarr using rechunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chunk = 72\n",
    "feature_chunk = 30000\n",
    "max_mem = '1.5GB'    # about 75% of dask worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk only 2D variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_plan={}\n",
    "for var in ds.data_vars:\n",
    "    if len(ds[var].dims)==2:\n",
    "        var_chunk = (time_chunk, feature_chunk)\n",
    "        chunk_plan[var] = var_chunk\n",
    "print(chunk_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close();client.close();\n",
    "#cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = LocalCluster(); client=Client(cluster); cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztemp = 's3://esip-qhub/noaa/NWM2/tmp4'\n",
    "zf = 's3://esip-qhub/noaa/NWM2/zarr4'\n",
    "\n",
    "#fs.rm(ztemp, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs.rm(zf, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_temp = fsspec.get_mapper(ztemp)\n",
    "zarr_chunked = fsspec.get_mapper(zf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_temp = './tmp3'\n",
    "zarr_chunked = './zarr3'\n",
    "    try:\n",
    "        shutil.rmtree(zarr_temp)\n",
    "        while os.path.exists(zarr_temp): # check if it still exists\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(zarr_chunked)\n",
    "        while os.path.exists(zarr_chunked): # check if it still exists\n",
    "            pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_plan = rechunk(ds_test, chunk_plan, max_mem, zarr_chunked, \n",
    "                     temp_store=zarr_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    result = array_plan.execute(retries=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read resulting Zarr file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_test = zarr.open_consolidated(fsspec.get_mapper('s3://coastalcoupling/noaa/NWM2/test_zarr'),                                        mode='r')\n",
    "#ds_chunk = xr.open_zarr(fsspec.get_mapper('s3://coastalcoupling/noaa/NWM2/zarr3'))\n",
    "ds_chunk = xr.open_zarr('zarr3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_chunk.streamflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster scale down\n",
    "\n",
    "When we are temporarily done with the cluster we can scale it down to save on costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_type == 'Fargate':\n",
    "    numWorkers=0\n",
    "    ecs.update_service(cluster=cluster, service='Dask-Worker', desiredCount=numWorkers)\n",
    "    ecs.get_waiter('services_stable').wait(cluster=cluster, services=['Dask-Worker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_type == 'Gateway':\n",
    "    cluster.scale(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rechunker]",
   "language": "python",
   "name": "conda-env-rechunker-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
