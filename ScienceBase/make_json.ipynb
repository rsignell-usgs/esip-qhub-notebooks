{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611b47e7-6efe-40a6-a14f-ead8e83c81db",
   "metadata": {},
   "source": [
    "## Create FileReferenceSystem JSON for NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a402b5f-ce17-4eb3-89b7-c530c3b448bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import Counter\n",
    "import zipfile\n",
    "from typing import Union, BinaryIO\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import zarr\n",
    "from zarr.meta import encode_fill_value\n",
    "# We don't usually use Shuffle and it's not in the current release:\n",
    "from numcodecs import Zlib #, Shuffle   \n",
    "import fsspec\n",
    "import fsspec.utils\n",
    "import fsspec.core\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f4614-b480-41a5-9c24-2330f7c430b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lggr = logging.getLogger('h5-to-zarr')\n",
    "\n",
    "\n",
    "class SingleHdf5ToZarr:\n",
    "    \"\"\"Translate the content of one HDF5 file into Zarr metadata.\n",
    "\n",
    "    HDF5 groups become Zarr groups. HDF5 datasets become Zarr arrays. Zarr array\n",
    "    chunks remain in the HDF5 file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h5f : file-like\n",
    "        Input HDF5 file as a binary Python file-like object (duck-typed, adhering\n",
    "        to BinaryIO is optional)\n",
    "    url : str\n",
    "        URI of the HDF5 file.\n",
    "    xarray : bool, optional\n",
    "        Produce attributes required by the `xarray <http://xarray.pydata.org>`_\n",
    "        package to correctly identify dimensions (HDF5 dimension scales) of a\n",
    "        Zarr array. Default is ``False``.\n",
    "    spec : int\n",
    "        The version of output to produce (see README of this repo)\n",
    "    inline_threshold : int\n",
    "        Include chunks smaller than this value directly in the output. Zero or negative\n",
    "        to disable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h5f: BinaryIO, url: str,\n",
    "                 xarray: bool = False, spec=1, inline_threshold=0):\n",
    "        # Open HDF5 file in read mode...\n",
    "        lggr.debug(f'HDF5 file: {h5f}')\n",
    "        self.input_file = h5f\n",
    "        lggr.debug(f'xarray: {xarray}')\n",
    "        self.spec = spec\n",
    "        self.inline = inline_threshold\n",
    "        self._h5f = h5py.File(h5f, mode='r')\n",
    "        self._xr = xarray\n",
    "\n",
    "        self.store = {}\n",
    "        self._zroot = zarr.group(store=self.store, overwrite=True)\n",
    "\n",
    "        self._uri = url\n",
    "        lggr.debug(f'HDF5 file URI: {self._uri}')\n",
    "\n",
    "    def translate(self):\n",
    "        \"\"\"Translate content of one HDF5 file into Zarr storage format.\n",
    "\n",
    "        This method is the main entry point to execute the workflow, and\n",
    "        returns a \"reference\" structure to be used with zarr/fsspec-reference-maker\n",
    "\n",
    "        No data is copied out of the HDF5 file.\n",
    "\n",
    "        :returns\n",
    "        dict with references\n",
    "        \"\"\"\n",
    "        lggr.debug('Translation begins')\n",
    "        self._transfer_attrs(self._h5f, self._zroot)\n",
    "        self._h5f.visititems(self._translator)\n",
    "        if self.inline > 0:\n",
    "            self._do_inline(self.inline)\n",
    "        if self.spec < 1:\n",
    "            return self.store\n",
    "        else:\n",
    "            for k, v in self.store.copy().items():\n",
    "                if isinstance(v, list):\n",
    "                    self.store[k][0] = \"{{u}}\"\n",
    "                else:\n",
    "                    self.store[k] = v.decode()\n",
    "            return {\n",
    "                \"version\": 1,\n",
    "                \"templates\": {\n",
    "                    \"u\": self._uri\n",
    "                },\n",
    "                \"refs\": self.store\n",
    "            }\n",
    "\n",
    "    def _do_inline(self, threshold):\n",
    "        \"\"\"Replace short chunks with the value of that chunk\n",
    "\n",
    "        The chunk may need encoding with base64 if not ascii, so actual\n",
    "        length may be larger than threshold.\n",
    "        \"\"\"\n",
    "        for k, v in self.store.copy().items():\n",
    "            if isinstance(v, list) and v[2] < threshold:\n",
    "                self.input_file.seek(v[1])\n",
    "                data = self.input_file.read(v[2])\n",
    "                try:\n",
    "                    # easiest way to test if data is ascii\n",
    "                    data.decode('ascii')\n",
    "                except UnicodeDecodeError:\n",
    "                    data = b\"base64:\" + base64.b64encode(data)\n",
    "                self.store[k] = data\n",
    "\n",
    "    def _transfer_attrs(self, h5obj: Union[h5py.Dataset, h5py.Group],\n",
    "                        zobj: Union[zarr.Array, zarr.Group]):\n",
    "        \"\"\"Transfer attributes from an HDF5 object to its equivalent Zarr object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h5obj : h5py.Group or h5py.Dataset\n",
    "            An HDF5 group or dataset.\n",
    "        zobj : zarr.hierarchy.Group or zarr.core.Array\n",
    "            An equivalent Zarr group or array to the HDF5 group or dataset with\n",
    "            attributes.\n",
    "        \"\"\"\n",
    "        for n, v in h5obj.attrs.items():\n",
    "            if n in ('REFERENCE_LIST', 'DIMENSION_LIST'):\n",
    "                continue\n",
    "\n",
    "            # Fix some attribute values to avoid JSON encoding exceptions...\n",
    "            if isinstance(v, bytes):\n",
    "                v = v.decode('utf-8')\n",
    "            elif isinstance(v, (np.ndarray, np.number)):\n",
    "                if v.dtype.kind == 'S':\n",
    "                    v = v.astype(str)\n",
    "                if n == '_FillValue':\n",
    "                    v = encode_fill_value(v, v.dtype)\n",
    "                elif v.size == 1:\n",
    "                    v = v.flatten()[0]\n",
    "                    if isinstance(v, (np.ndarray, np.number)):\n",
    "                        v = v.tolist()\n",
    "                else:\n",
    "                    v = v.tolist()\n",
    "            if self._xr and v == 'DIMENSION_SCALE':\n",
    "                continue\n",
    "            try:\n",
    "                zobj.attrs[n] = v\n",
    "            except TypeError:\n",
    "                lggr.exception(\n",
    "                    f'Caught TypeError: {n}@{h5obj.name} = {v} ({type(v)})')\n",
    "\n",
    "    def _translator(self, name: str, h5obj: Union[h5py.Dataset, h5py.Group]):\n",
    "        \"\"\"Produce Zarr metadata for all groups and datasets in the HDF5 file.\n",
    "        \"\"\"\n",
    "        refs = {}\n",
    "        if isinstance(h5obj, h5py.Dataset):\n",
    "            lggr.debug(f'HDF5 dataset: {h5obj.name}')\n",
    "            if h5obj.id.get_create_plist().get_layout() == h5py.h5d.COMPACT:\n",
    "                RuntimeError(\n",
    "                    f'Compact HDF5 datasets not yet supported: <{h5obj.name} '\n",
    "                    f'{h5obj.shape} {h5obj.dtype} {h5obj.nbytes} bytes>')\n",
    "                return\n",
    "\n",
    "            if (h5obj.scaleoffset or h5obj.fletcher32 or\n",
    "                    h5obj.compression in ('szip', 'lzf')):\n",
    "                raise RuntimeError(\n",
    "                    f'{h5obj.name} uses unsupported HDF5 filters')\n",
    "            if h5obj.compression == 'gzip':\n",
    "                compression = Zlib(level=h5obj.compression_opts)\n",
    "            else:\n",
    "                compression = None\n",
    "            \n",
    "            # Add filter for shuffle\n",
    "            filters = []\n",
    "            if h5obj.shuffle:\n",
    "                filters.append(Shuffle(elementsize=h5obj.dtype.itemsize))\n",
    "\n",
    "            # Get storage info of this HDF5 dataset...\n",
    "            cinfo = self._storage_info(h5obj)\n",
    "            if self._xr and h5py.h5ds.is_scale(h5obj.id) and not cinfo:\n",
    "                return\n",
    "\n",
    "            # Create a Zarr array equivalent to this HDF5 dataset...\n",
    "            za = self._zroot.create_dataset(h5obj.name, shape=h5obj.shape,\n",
    "                                            dtype=h5obj.dtype,\n",
    "                                            chunks=h5obj.chunks or False,\n",
    "                                            fill_value=h5obj.fillvalue,\n",
    "                                            compression=compression,\n",
    "                                            filters=filters,\n",
    "                                            overwrite=True)\n",
    "            lggr.debug(f'Created Zarr array: {za}')\n",
    "            self._transfer_attrs(h5obj, za)\n",
    "\n",
    "            if self._xr:\n",
    "                # Do this for xarray...\n",
    "                adims = self._get_array_dims(h5obj)\n",
    "                za.attrs['_ARRAY_DIMENSIONS'] = adims\n",
    "                lggr.debug(f'_ARRAY_DIMENSIONS = {adims}')\n",
    "\n",
    "            # Store chunk location metadata...\n",
    "            if cinfo:\n",
    "                for k, v in cinfo.items():\n",
    "                    self.store[za._chunk_key(k)] = [self._uri, v['offset'], v['size']]\n",
    "\n",
    "        elif isinstance(h5obj, h5py.Group):\n",
    "            lggr.debug(f'HDF5 group: {h5obj.name}')\n",
    "            zgrp = self._zroot.create_group(h5obj.name)\n",
    "            self._transfer_attrs(h5obj, zgrp)\n",
    "\n",
    "    def _get_array_dims(self, dset):\n",
    "        \"\"\"Get a list of dimension scale names attached to input HDF5 dataset.\n",
    "\n",
    "        This is required by the xarray package to work with Zarr arrays. Only\n",
    "        one dimension scale per dataset dimension is allowed. If dataset is\n",
    "        dimension scale, it will be considered as the dimension to itself.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dset : h5py.Dataset\n",
    "            HDF5 dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List with HDF5 path names of dimension scales attached to input\n",
    "            dataset.\n",
    "        \"\"\"\n",
    "        dims = list()\n",
    "        rank = len(dset.shape)\n",
    "        if rank:\n",
    "            for n in range(rank):\n",
    "                num_scales = len(dset.dims[n])\n",
    "                if num_scales == 1:\n",
    "                    dims.append(dset.dims[n][0].name[1:])\n",
    "                elif h5py.h5ds.is_scale(dset.id):\n",
    "                    dims.append(dset.name[1:])\n",
    "                elif num_scales > 1:\n",
    "                    raise RuntimeError(\n",
    "                        f'{dset.name}: {len(dset.dims[n])} '\n",
    "                        f'dimension scales attached to dimension #{n}')\n",
    "        return dims\n",
    "\n",
    "    def _storage_info(self, dset: h5py.Dataset) -> dict:\n",
    "        \"\"\"Get storage information of an HDF5 dataset in the HDF5 file.\n",
    "\n",
    "        Storage information consists of file offset and size (length) for every\n",
    "        chunk of the HDF5 dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dset : h5py.Dataset\n",
    "            HDF5 dataset for which to collect storage information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            HDF5 dataset storage information. Dict keys are chunk array offsets\n",
    "            as tuples. Dict values are pairs with chunk file offset and size\n",
    "            integers.\n",
    "        \"\"\"\n",
    "        # Empty (null) dataset...\n",
    "        if dset.shape is None:\n",
    "            return dict()\n",
    "\n",
    "        dsid = dset.id\n",
    "        if dset.chunks is None:\n",
    "            # Contiguous dataset...\n",
    "            if dsid.get_offset() is None:\n",
    "                # No data ever written...\n",
    "                return dict()\n",
    "            else:\n",
    "                key = (0,) * (len(dset.shape) or 1)\n",
    "                return {key: {'offset': dsid.get_offset(),\n",
    "                              'size': dsid.get_storage_size()}}\n",
    "        else:\n",
    "            # Chunked dataset...\n",
    "            num_chunks = dsid.get_num_chunks()\n",
    "            if num_chunks == 0:\n",
    "                # No data ever written...\n",
    "                return dict()\n",
    "\n",
    "            # Go over all the dataset chunks...\n",
    "            stinfo = dict()\n",
    "            chunk_size = dset.chunks\n",
    "            for index in range(num_chunks):\n",
    "                blob = dsid.get_chunk_info(index)\n",
    "                key = tuple(\n",
    "                    [a // b for a, b in zip(blob.chunk_offset, chunk_size)])\n",
    "                stinfo[key] = {'offset': blob.byte_offset, 'size': blob.size}\n",
    "            return stinfo\n",
    "\n",
    "\n",
    "class MultiZarrToZarr:\n",
    "\n",
    "    def __init__(self, path, remote_protocol,\n",
    "                 remote_options=None, xarray_kwargs=None, storage_options=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param path: a URL containing multiple JSONs\n",
    "        :param xarray_kwargs:\n",
    "        :param storage_options:\n",
    "        \"\"\"\n",
    "        xarray_kwargs = xarray_kwargs or {}\n",
    "        self.path = path\n",
    "        self.xr_kwargs = xarray_kwargs\n",
    "        self.storage_options = storage_options or {}\n",
    "        self.remote_protocol = remote_protocol\n",
    "        self.remote_options = remote_options or {}\n",
    "\n",
    "    def translate(self, outpath):\n",
    "        ds, ds0, fss = self._determine_dims()\n",
    "        out = self._build_output(ds, ds0, fss)\n",
    "        self.output = self._consolidate(out)\n",
    "        self._write(self.output, outpath)\n",
    "\n",
    "    @staticmethod\n",
    "    def _write(refs, outpath, filetype=None):\n",
    "        types = {\n",
    "            \"json\": \"json\",\n",
    "            \"parquet\": \"parquet\",\n",
    "            \"zarr\": \"zarr\"\n",
    "        }\n",
    "        if filetype is None:\n",
    "            ext = os.path.splitext(outpath)[1].lstrip(\".\")\n",
    "            filetype = types[ext]\n",
    "        elif filetype not in types:\n",
    "            raise KeyError\n",
    "        if filetype == \"json\":\n",
    "            with open(outpath, \"w\") as f:\n",
    "                json.dump(refs, f)\n",
    "            return\n",
    "        import pandas as pd\n",
    "        references2 = {\n",
    "            k: {\"data\": v.encode('ascii') if not isinstance(v, list) else None,\n",
    "                \"url\": v[0] if isinstance(v, list) else None,\n",
    "                \"offset\": v[1] if isinstance(v, list) else None,\n",
    "                \"size\": v[2] if isinstance(v, list) else None}\n",
    "            for k, v in refs['refs'].items()}\n",
    "        # use pandas for sorting\n",
    "        df = pd.DataFrame(references2.values(), index=list(references2)).sort_values(\"offset\")\n",
    "\n",
    "        if filetype == \"zarr\":\n",
    "            import zarr\n",
    "            import numcodecs\n",
    "            # compression should be NONE, if intent is to store in single zip\n",
    "            g = zarr.open_group(outpath, mode='w')\n",
    "            g.attrs.update({k: v for k, v in refs.items() if k in ['version', \"templates\", \"gen\"]})\n",
    "            g.array(name=\"key\", data=df.index.values, dtype=\"object\", compression=\"zstd\",\n",
    "                    object_codec=numcodecs.VLenUTF8())\n",
    "            g.array(name=\"offset\", data=df.offset.values, dtype=\"uint32\", compression=\"zstd\")\n",
    "            g.array(name=\"size\", data=df['size'].values, dtype=\"uint32\", compression=\"zstd\")\n",
    "            g.array(name=\"data\", data=df.data.values, dtype=\"object\",\n",
    "                    object_codec=numcodecs.VLenBytes(), compression=\"gzip\")\n",
    "            # may be better as fixed length\n",
    "            g.array(name=\"url\", data=df.url.values, dtype=\"object\",\n",
    "                    object_codec=numcodecs.VLenUTF8(), compression='gzip')\n",
    "        if filetype == \"parquet\":\n",
    "            import fastparquet\n",
    "            metadata = {k: v for k, v in refs.items() if k in ['version', \"templates\", \"gen\"]}\n",
    "            fastparquet.write(\n",
    "                outpath,\n",
    "                df,\n",
    "                custom_metadata=metadata,\n",
    "                compression=\"ZSTD\"\n",
    "            )\n",
    "\n",
    "    def _consolidate(self, mapping, inline_threashold=100, template_count=5):\n",
    "        import string\n",
    "        counts = Counter(v[0] for v in mapping.values() if isinstance(v, list))\n",
    "        # potential IndexError when more than 52 templates\n",
    "        templates = {f\"{string.ascii_letters[i]}\": u for i, (u, v) in enumerate(counts.items())\n",
    "                     if v > template_count}\n",
    "        inv = {v: k for k, v in templates.items()}\n",
    "\n",
    "        out = {}\n",
    "        for k, v in mapping.items():\n",
    "            if isinstance(v, list) and v[2] < inline_threashold:\n",
    "                v = self.fs.cat_file(v[0], v[1], v[1] + v[2])\n",
    "            if isinstance(v, bytes):\n",
    "                try:\n",
    "                    # easiest way to test if data is ascii\n",
    "                    out[k] = v.decode('ascii')\n",
    "                except UnicodeDecodeError:\n",
    "                    out[k] = (b\"base64:\" + base64.b64encode(v)).decode()\n",
    "            else:\n",
    "                if v[0] in inv:\n",
    "                    out[k] = [\"{{\" + inv[v[0]] + \"}}\"] + v[1:]\n",
    "                else:\n",
    "                    out[k] = v\n",
    "        return {\"version\": 1, \"templates\": templates, \"refs\": out}\n",
    "\n",
    "    def _build_output(self, ds, ds0, fss):\n",
    "        import zarr\n",
    "        out = {}\n",
    "        ds.to_zarr(out, chunk_store={}, compute=False)  # fills in metadata&coords\n",
    "        z = zarr.open_group(out, mode='a')\n",
    "        for dim in self.extra_dims.union(self.concat_dims):\n",
    "            # derived and concatenated dims stored as absolute data\n",
    "            z[dim][:] = ds[dim].values\n",
    "        for dim in self.same_dims:\n",
    "            # duplicated coordinates stored as references just once\n",
    "            out.update({k: v for k, v in fss[0].references.items() if k.startswith(dim)})\n",
    "        for variable in ds.variables:\n",
    "            if variable in ds.dims:\n",
    "                # already handled\n",
    "                continue\n",
    "            var, var0 = ds[variable], ds0[variable]\n",
    "            assert var.dims[-len(var0.dims):] == var0.dims\n",
    "\n",
    "            concats = {d: 0 for d in self.concat_dims}\n",
    "            for i, fs in enumerate(fss):\n",
    "                for k, v in fs.references.items():\n",
    "                    start, part = os.path.split(k)\n",
    "                    if start != variable or part in ['.zgroup', '.zarray', '.zattrs']:\n",
    "                        # OK, so we go through all the keys multiple times\n",
    "                        continue\n",
    "                    if var.shape == var0.shape:\n",
    "                        out[k] = v  # copy\n",
    "                    else:\n",
    "                        out[f\"{start}/{i}.{part}\"] = v\n",
    "        return out\n",
    "\n",
    "    def _determine_dims(self):\n",
    "        import xarray as xr\n",
    "        with fsspec.open_files(self.path, **self.storage_options) as ofs:\n",
    "            fss = [\n",
    "                fsspec.filesystem(\n",
    "                    \"reference\", fo=json.load(of),\n",
    "                    remote_protocol=self.remote_protocol,\n",
    "                    remote_options=self.remote_options\n",
    "                ) for of in ofs\n",
    "            ]\n",
    "            self.fs = fss[0].fs\n",
    "            mappers = [fs.get_mapper(\"\") for fs in fss]\n",
    "\n",
    "        ds = xr.open_mfdataset(mappers, concat_dim='time', engine=\"zarr\", chunks={}, **self.xr_kwargs)\n",
    "        ds0 = xr.open_mfdataset(mappers[:1], concat_dim='time', engine=\"zarr\", chunks={}, **self.xr_kwargs)\n",
    "        self.extra_dims = set(ds.dims) - set(ds0.dims)\n",
    "        self.concat_dims = set(k for k, v in ds.dims.items() if v / ds0.dims[k] == len(mappers))\n",
    "        self.same_dims = set(ds.dims) - self.extra_dims - self.concat_dims\n",
    "        return ds, ds0, fss\n",
    "\n",
    "\n",
    "def example_single():\n",
    "    \"\"\"Scans the given file and returns a dict of references\"\"\"\n",
    "    url = 's3://pangeo-data-uswest2/esip/adcirc/adcirc_01d.nc'\n",
    "    so = dict(\n",
    "        mode='rb', anon=False, requester_pays=True,\n",
    "        default_fill_cache=False, default_cache_type='none'\n",
    "    )\n",
    "    fsspec.utils.setup_logging(logger=lggr)\n",
    "    with fsspec.open(url, **so) as f:\n",
    "        h5chunks = SingleHdf5ToZarr(f, url, xarray=True)\n",
    "        return h5chunks.translate()\n",
    "\n",
    "\n",
    "def hdf5_multiple(urls, so):\n",
    "    \"\"\"Scans the set of URLs and writes a reference JSON file\n",
    "\n",
    "    In this prototype, the outputs are wrapped in a single ZIP archive\n",
    "    \"out.zip\".\n",
    "    \"\"\"\n",
    "\n",
    "    zf = zipfile.ZipFile(\"out.zip\", mode=\"w\")\n",
    "    for u in urls:\n",
    "        with fsspec.open(u, **so) as inf:\n",
    "            h5chunks = SingleHdf5ToZarr(inf, u, xarray=True, inline_threshold=100)\n",
    "            with zf.open(os.path.basename(u) + \".json\", 'w') as outf:\n",
    "                outf.write(json.dumps(h5chunks.translate()).encode())\n",
    "\n",
    "\n",
    "def example_ensemble(ofile):\n",
    "    \"\"\"Scan the set of URLs and create a single reference output\n",
    "    This example uses the output of example_multiple\n",
    "    \"\"\"\n",
    "    \n",
    "    mzz = MultiZarrToZarr(\n",
    "        \"zip://*.json::out.zip\",\n",
    "        remote_protocol=\"s3\",\n",
    "        remote_options={'anon': True},\n",
    "        xarray_kwargs={\n",
    "            \"decode_cf\": False,\n",
    "            \"mask_and_scale\": False,\n",
    "            \"decode_times\": False,\n",
    "            \"decode_timedelta\": False,\n",
    "            \"use_cftime\": False,\n",
    "            \"decode_coords\": False\n",
    "        },\n",
    "    )\n",
    "    mzz.translate(ofile)\n",
    "\n",
    "# try GOM1km NetCDF4/HDF5 files in requester pays bucket\n",
    "\n",
    "so = dict(\n",
    "    anon=False, requester_pays=True, default_fill_cache=False, default_cache_type='first'\n",
    ")\n",
    "\n",
    "def example_sciencebase(url,ofile ):\n",
    "    \"\"\"Scans the given file and returns a dict of references\"\"\"\n",
    "    so = dict(\n",
    "        mode='rb', anon=True, default_fill_cache=False, default_cache_type='none'\n",
    "    )\n",
    "#    fsspec.utils.setup_logging(logger=lggr)\n",
    "    with fsspec.open(url, **so) as f:\n",
    "        h5chunks = SingleHdf5ToZarr(f, url, xarray=True)\n",
    "        json_data = h5chunks.translate()\n",
    "        with open(ofile, 'w') as f:\n",
    "            json.dump(json_data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e156b27-4b7b-4021-9167-56bc15034f03",
   "metadata": {},
   "source": [
    "Create the JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46e6c0-c0de-4e6b-9f61-3b5a806b422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 's3://prod-is-usgs-sb-prod-publish/609bf69ed34ea221ce39b261/breach_matanzas.nc'\n",
    "ncfile = url.split('/')[-1]\n",
    "jsonfile = ncfile.replace('.nc','.json')\n",
    "example_sciencebase(url, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0fa2e-1b2c-4686-8b91-aa84292a6e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 's3://prod-is-usgs-sb-prod-publish/609bf69ed34ea221ce39b261/breach_wilderness_zovary.nc'\n",
    "ncfile = url.split('/')[-1]\n",
    "jsonfile = ncfile.replace('.nc','.json')\n",
    "example_sciencebase(url, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485e68d-9714-4b7c-8778-d9f422a460fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 's3://prod-is-usgs-sb-prod-publish/609bf69ed34ea221ce39b261/breach_wilderness_zonovary.nc'\n",
    "ncfile = url.split('/')[-1]\n",
    "jsonfile = ncfile.replace('.nc','.json')\n",
    "example_sciencebase(url, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adb907-e5ef-485f-8c89-45dc51ac4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 's3://prod-is-usgs-sb-prod-publish/609bf69ed34ea221ce39b261/breach_wilderness_veg.nc'\n",
    "ncfile = url.split('/')[-1]\n",
    "jsonfile = ncfile.replace('.nc','.json')\n",
    "example_sciencebase(url, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4fe97-f2dc-4c09-a373-07d338fe0dc9",
   "metadata": {},
   "source": [
    "See if the JSON works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f2e7e-0c0a-4663-a00b-55c1540339ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "r_opts = {'anon':True} # NetCDF files \n",
    "fo = \"breach_wilderness_zovary.json\"\n",
    "fs = fsspec.filesystem(\"reference\", fo=fo, \n",
    "                       remote_protocol='s3', remote_options=r_opts)\n",
    "m = fs.get_mapper(\"\")\n",
    "ds = xr.open_dataset(m, engine=\"zarr\", decode_timedelta=False, \n",
    "                     chunks={'ocean_time':200, 'eta_rho':300, 'xi_rho':400})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f114d-0881-40c4-aa35-e687b7005b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5158c89-d86a-4880-ab2a-c986d9fe18e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
