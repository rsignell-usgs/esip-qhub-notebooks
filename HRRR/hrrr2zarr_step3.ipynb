{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HRRR to Zarr\n",
    "#### Step 3/3: Aggregate NetCDF files, rechunk and store as Zarr\n",
    "Rechunk a collection of HRRR NetCDF files (converted from GRIB2 using \"wgrib2\") and convert to Zarr using xarray and rechunker.  We process the first time chunk, write to zarr, then repeat the process for the rest of the time chunks, appending each one. We use xr.open_mfdataset and rechunker on each time chunk, except for the last partial time chunk, where rechunker bombs.  So we rechunk that last partial step without using rechunker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, performance_report\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "import os\n",
    "# according to Rich Brey at WHOI, the are 160GB available on each 36 core node\n",
    "# on poseidon, but need to leave some for system memory (here we leave 16GB)\n",
    "cluster = SLURMCluster(processes=1, cores=36, memory='144GB',\n",
    "                    walltime='02:00:00', queue='compute')\n",
    "\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(4)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = Client()\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset('./nc/hrrr.20190101*.nc', chunks={'time':1}, concat_dim='time', \n",
    "combine='nested', coords='minimal', compat='override', parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ds.time.diff(dim='time'))/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rechunker import rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mem = '2.5GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chunk_size = 144\n",
    "x_chunk_size = 300\n",
    "y_chunk_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "300*300*144*4/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close(); client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_step = '/vortexfs1/usgs/rsignell/HRRR/zarr/step'\n",
    "zarr_temp = '/vortexfs1/usgs/rsignell/HRRR/zarr/tmp'\n",
    "zarr_chunked = '/vortexfs1/usgs/rsignell/HRRR/zarr/hrrr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunk_plan={}\n",
    "for var in ds.data_vars:\n",
    "    if len(ds[var].dims)==3:\n",
    "        var_chunk = (time_chunk_size, y_chunk_size, x_chunk_size)\n",
    "        chunk_plan[var] = var_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(\"./nc/hrrr.2019*.nc\")\n",
    "files = np.sort(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_chunks = int(np.ceil(len(files)/time_chunk_size))\n",
    "nt_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(60,nt_chunks):\n",
    "    print(i)\n",
    "    istart = i * time_chunk_size\n",
    "    istop = int(np.min([(i+1) * time_chunk_size, len(files)]))\n",
    "    \n",
    "    ds = xr.open_mfdataset(files[istart:istop], concat_dim='time', \n",
    "                           combine='by_coords', coords='minimal', \n",
    "                           compat='override', parallel=True)\n",
    "       \n",
    "    # chunk this step to zarr using rechunker\n",
    "\n",
    "    # remote the temp and step zarr datasets\n",
    "    try:\n",
    "        shutil.rmtree(zarr_temp, ignore_errors=False, onerror=None)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        shutil.rmtree(zarr_step, ignore_errors=False, onerror=None)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    chunk_plan={}\n",
    "    for var in ds.data_vars:\n",
    "        if len(ds[var].dims)==3:\n",
    "            var_chunk = (time_chunk_size, y_chunk_size, x_chunk_size)\n",
    "            chunk_plan[var] = var_chunk\n",
    "\n",
    "    array_plan = rechunk(ds, chunk_plan, max_mem, zarr_step, \n",
    "                     temp_store=zarr_temp)\n",
    "    \n",
    "    with performance_report(filename=\"dask-report.html\"):\n",
    "        result = array_plan.execute(retries=10)\n",
    "\n",
    "    # read back in the zarr chunk rechunker wrote\n",
    "    ds = xr.open_zarr(zarr_step)\n",
    "\n",
    "    if i==0:\n",
    "        ds.to_zarr(zarr_chunked, consolidated=True, mode='w')\n",
    "    else:\n",
    "        ds.to_zarr(zarr_chunked, consolidated=True, append_dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the last partial chunk, not using rechunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.chunk({'x':x_chunk_size,'y':y_chunk_size, 'time':time_chunk_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.to_zarr('./zarr/last_step', consolidated=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = xr.open_zarr('./zarr/last_step', consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.to_zarr(zarr_chunked, consolidated=True, append_dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close(); cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
