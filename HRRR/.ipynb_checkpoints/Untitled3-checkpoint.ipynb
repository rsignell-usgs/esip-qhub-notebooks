{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f29d19-da69-4c19-b17b-94d813e00052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import cfgrib\n",
    "import numcodecs.abc\n",
    "from numcodecs.compat import ndarray_copy, ensure_contiguous_ndarray\n",
    "import fsspec\n",
    "import zarr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c783a-c2ac-4d8f-a6c1-d7284fc22b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"grib2-to-zarr\")\n",
    "\n",
    "\n",
    "def _split_file(f, skip=0):\n",
    "    if hasattr(f, \"size\"):\n",
    "        size = f.size\n",
    "    else:\n",
    "        f.seek(0, 2)\n",
    "        size = f.tell()\n",
    "        f.seek(0)\n",
    "    part = 0\n",
    "\n",
    "    while f.tell() < size:\n",
    "        logger.debug(f\"extract part {part}\")\n",
    "        start = f.tell()\n",
    "        f.seek(12, 1)\n",
    "        part_size = int.from_bytes(f.read(4), \"big\")\n",
    "        f.seek(start)\n",
    "        data = f.read(part_size)\n",
    "        assert data[:4] == b\"GRIB\"\n",
    "        assert data[-4:] == b\"7777\"\n",
    "        fn = tempfile.mktemp(suffix=\"grib2\")\n",
    "        with open(fn, \"wb\") as fo:\n",
    "            fo.write(data)\n",
    "        yield fn, start, part_size\n",
    "        part += 1\n",
    "        if skip and part > skip:\n",
    "            break\n",
    "\n",
    "\n",
    "def _store_array(store, z, data, var, inline_threshold, offset, size, attr):\n",
    "    nbytes = data.dtype.itemsize\n",
    "    for i in data.shape:\n",
    "        nbytes *= i\n",
    "\n",
    "    shape = tuple(data.shape or ())\n",
    "    if nbytes < inline_threshold:\n",
    "        logger.debug(f\"Store {var} inline\")\n",
    "        d = z.create_dataset(\n",
    "            name=var,\n",
    "            shape=shape,\n",
    "            chunks=shape,\n",
    "            dtype=data.dtype,\n",
    "            fill_value=getattr(data, \"missing_value\", 0),\n",
    "            compressor=False,\n",
    "        )\n",
    "        if hasattr(data, \"tobytes\"):\n",
    "            b = data.tobytes()\n",
    "        else:\n",
    "            b = data.build_array().tobytes()\n",
    "        try:\n",
    "            # easiest way to test if data is ascii\n",
    "            b.decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            b = b\"base64:\" + base64.b64encode(data)\n",
    "        store[f\"{var}/0\"] = b.decode('ascii')\n",
    "    else:\n",
    "        logger.debug(f\"Store {var} reference\")\n",
    "        d = z.create_dataset(\n",
    "            name=var,\n",
    "            shape=shape,\n",
    "            chunks=shape,\n",
    "            dtype=data.dtype,\n",
    "            fill_value=getattr(data, \"missing_value\", 0),\n",
    "            filters=[GRIBCodec(var=var)],\n",
    "            compressor=False,\n",
    "        )\n",
    "        store[f\"{var}/\" + \".\".join([\"0\"] * len(shape))] = [\"{{u}}\", offset, size]\n",
    "    d.attrs.update(attr)\n",
    "\n",
    "\n",
    "def scan_grib(url, common_vars, storage_options, inline_threshold=100, skip=0, filter={}):\n",
    "    if filter:\n",
    "        assert \"typeOfLevel\" in filter and \"level\" in filter\n",
    "    logger.debug(f\"Open {url}\")\n",
    "\n",
    "    store = {}\n",
    "    z = zarr.open_group(store, mode='w')\n",
    "    common = False\n",
    "    with fsspec.open(url, \"rb\", **storage_options) as f:\n",
    "        for fn, offset, size in _split_file(f, skip=skip):\n",
    "            logger.debug(f\"File {fn}\")\n",
    "            ds = cfgrib.open_file(fn)\n",
    "            if filter:\n",
    "                var = filter[\"typeOfLevel\"]\n",
    "                if var not in ds.variables:\n",
    "                    continue\n",
    "                if ds.variables[var].data != filter[\"level\"]:\n",
    "                    continue\n",
    "                if common is False:\n",
    "                    # done for first valid message\n",
    "                    logger.debug(\"Common variables\")\n",
    "                    z.attrs.update(ds.attributes)\n",
    "                    for var in common_vars:\n",
    "                        # assume grid, etc is the same across all messages\n",
    "                        attr = ds.variables[var].attributes or {}\n",
    "                        attr['_ARRAY_DIMENSIONS'] = ds.variables[var].dimensions\n",
    "                        _store_array(store, z, ds.variables[var].data, var, inline_threshold, offset, size,\n",
    "                                     attr)\n",
    "                    common = True\n",
    "                if var not in z:\n",
    "                    attr = ds.variables[var].attributes or {}\n",
    "                    attr['_ARRAY_DIMENSIONS'] = []\n",
    "                    _store_array(store, z, np.array(filter[\"level\"]), var, 100000, 0, 0,\n",
    "                                 attr)\n",
    "\n",
    "            for var in ds.variables:\n",
    "                if var not in common_vars and getattr(ds.variables[var].data, \"shape\", None):\n",
    "\n",
    "                    attr = ds.variables[var].attributes or {}\n",
    "                    attr['_ARRAY_DIMENSIONS'] = ds.variables[var].dimensions\n",
    "                    _store_array(store, z, ds.variables[var].data, var, inline_threshold, offset, size,\n",
    "                         attr)\n",
    "    logger.debug(\"Done\")\n",
    "    return {\"version\": 1,\n",
    "            \"refs\": {k: v.decode() if isinstance(v, bytes) else v for k, v in store.items()},\n",
    "            \"templates\": {\"u\": url}}\n",
    "\n",
    "\n",
    "class GRIBCodec(numcodecs.abc.Codec):\n",
    "    \"\"\"\n",
    "    Read GRIB stream of bytes by writing to a temp file and calling cfgrib\n",
    "    \"\"\"\n",
    "\n",
    "    codec_id = 'grib'\n",
    "\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def encode(self, buf):\n",
    "        # on encode, pass through\n",
    "        return buf\n",
    "\n",
    "    def decode(self, buf, out=None):\n",
    "        buf = ensure_contiguous_ndarray(buf)\n",
    "        fn = tempfile.mktemp(suffix=\"grib2\")\n",
    "        buf.tofile(fn)\n",
    "\n",
    "        # do decode\n",
    "        ds = cfgrib.open_file(fn)\n",
    "        data = ds.variables[self.var].data\n",
    "        if hasattr(data, \"build_array\"):\n",
    "            data = data.build_array()\n",
    "\n",
    "        if out is not None:\n",
    "            return ndarray_copy(data, out)\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "\n",
    "numcodecs.register_codec(GRIBCodec, \"grib\")\n",
    "\n",
    "\n",
    "def example_multi():\n",
    "    import json\n",
    "    # 1GB of data files, forming a time-series\n",
    "    files = ['s3://noaa-hrrr-bdp-pds/hrrr.20190101/conus/hrrr.t22z.wrfsfcf01.grib2',\n",
    "             's3://noaa-hrrr-bdp-pds/hrrr.20190101/conus/hrrr.t23z.wrfsfcf01.grib2',\n",
    "             's3://noaa-hrrr-bdp-pds/hrrr.20190102/conus/hrrr.t00z.wrfsfcf01.grib2',\n",
    "             's3://noaa-hrrr-bdp-pds/hrrr.20190102/conus/hrrr.t01z.wrfsfcf01.grib2',\n",
    "             's3://noaa-hrrr-bdp-pds/hrrr.20190102/conus/hrrr.t02z.wrfsfcf01.grib2',\n",
    "             's3://noaa-hrrr-bdp-pds/hrrr.20190102/conus/hrrr.t03z.wrfsfcf01.grib2',\n",
    "             's3://noaa-hrrr-bdp-pds/hrrr.20190102/conus/hrrr.t04z.wrfsfcf01.grib2',\n",
    "             's3://noaa-hrrr-bdp-pds/hrrr.20190102/conus/hrrr.t05z.wrfsfcf01.grib2',\n",
    "             's3://noaa-hrrr-bdp-pds/hrrr.20190102/conus/hrrr.t06z.wrfsfcf01.grib2']\n",
    "    so = {\"anon\": True, \"default_cache_type\": \"readahead\"}\n",
    "    common = ['time', 'step', 'latitude', 'longitude', 'valid_time']\n",
    "    filter = {'typeOfLevel': 'heightAboveGround', 'level': 2}\n",
    "    for url in files:\n",
    "        print(url)\n",
    "        out = scan_grib(url, common, so, inline_threshold=100, filter=filter)\n",
    "        with open(os.path.basename(url).replace(\"grib2\", \"json\"), \"w\") as f:\n",
    "            json.dump(out, f)\n",
    "\n",
    "    # stitch with\n",
    "    # files = ['hrrr.t22z.wrfsfcf01.json',\n",
    "    #  'hrrr.t23z.wrfsfcf01.json',\n",
    "    #  'hrrr.t00z.wrfsfcf01.json',\n",
    "    #  'hrrr.t01z.wrfsfcf01.json',\n",
    "    #  'hrrr.t02z.wrfsfcf01.json',\n",
    "    #  'hrrr.t03z.wrfsfcf01.json',\n",
    "    #  'hrrr.t04z.wrfsfcf01.json',\n",
    "    #  'hrrr.t05z.wrfsfcf01.json',\n",
    "    #  'hrrr.t06z.wrfsfcf01.json']\n",
    "    # mzz = MultiZarrToZarr(files, remote_protocol=\"s3\", remote_options={\"anon\": True}, with_mf='time')\n",
    "    # mzz.translate(\"hrrr.total.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d193b01-5641-4f39-b940-5ea5cc3d5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2033c-8efc-4190-bc2e-8c92aa73d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start='2019-01-01 22:00',end='2019-01-02 06:00', freq='1h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6bf29-bef4-4070-aff2-c20ae736fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date2url(date):\n",
    "    yyyymmdd = date.strftime('%Y%m%d')\n",
    "    hh = date.strftime('%H')\n",
    "    cfile = f's3://noaa-hrrr-bdp-pds/hrrr.{yyyymmdd}/conus/hrrr.t{hh}z.wrfsfcf01.grib2'\n",
    "    return cfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81baf4-7e74-4316-9890-2f1d10c8a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfiles = [date2url(date) for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287d294-8788-4b7c-8904-7f6700490241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "example_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f4773-66d6-4b07-aa8e-5d81cd4731b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
