{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HRRR to Zarr\n",
    "#### Step 2/3: Fill missing data gaps\n",
    "Here we use data from previous forecast cycles to fill any gaps that remain after trying to download the `tau=1 hour` data from each hourly forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fsspec\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import subprocess\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "import os\n",
    "cluster = SLURMCluster(processes=1, cores=36, memory='3GB',\n",
    "                    walltime='23:00:00', queue='compute')\n",
    "\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a different directory (here \"./nc2\") to store the grib and netcdf files used to fill the gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-hrrr-bdp-pds'  # archive'\n",
    "ldir = '/vortexfs1/usgs/rsignell/HRRR/nc2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.glob('s3://noaa-hrrr-bdp-pds/hrrr.20190301/conus/hrrr.t00z.wrfsfcf*.grib2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_file = fsspec.open_local(f'simplecache::s3://{flist[0]}', \n",
    "#                              s3=dict(anon=True), simplecache={'cache_storage': '/tmp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = xr.open_dataset(tmp_file, engine='cfgrib', \n",
    "#                       backend_kwargs=dict(filter_by_keys={'typeOfLevel': 'heightAboveGround', 'level': 2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start='2019-01-01 00:00',end='2019-12-31 23:00', freq='1h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check every time (every hour), and if a local grib file is not found, append that filename and time to a missing data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ldir0 = '/vortexfs1/usgs/rsignell/HRRR/nc'\n",
    "fmissing = []\n",
    "dmissing = []\n",
    "for date in dates:\n",
    "    yyyymmdd = date.strftime('%Y%m%d')\n",
    "    hh = date.strftime('%H')\n",
    "    s3file = f'{bucket}/hrrr.{yyyymmdd}/conus/hrrr.t{hh}z.wrfsfcf01.grib2'\n",
    "    fname = f'{ldir0}/hrrr.{yyyymmdd}{hh}.wrfsfcf01.grib2' \n",
    "    date\n",
    "    if not os.path.exists(fname):\n",
    "        fmissing.append(s3file)\n",
    "        dmissing.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dmissing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a date and an offset on input and trys to find a GRIB2 file that will supply the requested forecast from the four preceding longer forecasts (longer 36 hour HRRR forecasts are made on 0,6,12,18 hours.  The regular hourly forecasts are for 18 hours). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrrr_aws(date, foff):\n",
    "    n = date.hour + foff\n",
    "    foff6 = int(np.mod(n,6)) \n",
    "    date6 = date + datetime.timedelta(hours=(foff-foff6))\n",
    "    foff = foff + foff6 - 1\n",
    "    for thour in [6, 12, 18, 24]:\n",
    "        date_t = date6 - datetime.timedelta(hours=thour)\n",
    "        foff_t = foff + thour\n",
    "        hh = date_t.strftime('%H')\n",
    "        foff_tt = f'f{(foff_t):02d}'\n",
    "        yyyymmdd = date_t.strftime('%Y%m%d')\n",
    "        cfile = f's3://noaa-hrrr-bdp-pds/hrrr.{yyyymmdd}/conus/hrrr.t{hh}z.wrfsfc{foff_tt}.grib2'\n",
    "        flist = fs.ls(cfile)\n",
    "        if not flist:   # if file listing is empty, keep going\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "    return cfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a specific grib file and convert to NetCDF using wgrib2, saving only certain variables. Here \"cfile\" is the actual GRIB file we download to fill the gap (as determined by the hrrr_aws function), but we save that grib file with the same name it would have had if it were not missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmatch = \"(:TMP:2 m above ground:|:RH:2 m above ground:|:UGRD:10 m above ground:|:VGRD:10 m above ground:|:PRATE:surface:|:DSWRF:surface:|:DLWRF:surface:|:USWRF:surface:)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def hrrr_grib2nc(cfile, date):\n",
    "    yyyymmdd = date.strftime('%Y%m%d')\n",
    "    hh = date.strftime('%H')\n",
    "    fname = f'{ldir}/hrrr.{yyyymmdd}{hh}.wrfsfcf01.grib2' \n",
    "    print(fname)\n",
    "    if not os.path.exists(fname):\n",
    "        try:\n",
    "            fs.download(cfile,fname)\n",
    "            output = fname.replace(\".grib2\", \".nc\")\n",
    "            call = [\"wgrib2\", fname, \"-match\", vmatch, \"-netcdf\", output]\n",
    "            verbose=False\n",
    "            ret = False\n",
    "            # Not very robust check but will allow you to re-run everything in case a single file failed.\n",
    "            if not os.path.exists(output):\n",
    "                ret = subprocess.run(call, capture_output=True)\n",
    "            if verbose:\n",
    "                print(ret.stdout.decode())\n",
    "            if ret and ret.returncode == 0:\n",
    "                print(f\"Converted {fname} to {output}.\")\n",
    "        except:\n",
    "            print(f'{cfile} not found')\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take data from 1st forecast hour, not the 00 analysis time to allow time for dynamic adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tau = 1  \n",
    "tasks = [hrrr_grib2nc(hrrr_aws(date, tau), date) for date in dmissing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(tasks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close(); cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
