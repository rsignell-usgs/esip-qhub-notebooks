{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d741d57e-03a1-4c35-b6af-6273992cdb03",
   "metadata": {},
   "source": [
    "# Explore the National Water Model Reanalysis v2.1 \n",
    "Explore the NWM Reanalysis (1979-2020) NetCDF files (all 367,439 of them) on AWS as a single xarray dataset! \n",
    "The only new file we created was a JSON file that points to data chunks in the original NetCDF files that is then read with the [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) and [zarr](https://zarr.readthedocs.io/en/stable/) packages. \n",
    "\n",
    "See this [blog post](https://medium.com/pangeo/cloud-performant-netcdf4-hdf5-with-zarr-fsspec-and-intake-3d3a3e7cb935) for how this works. \n",
    "\n",
    "**Important note on performance**: The data in the original NetCDF files is chunked as the entire spatial domain and a single time step.  Thus reading a time series will be very slow -- to extract a time series at a single location for the entire time period will require reading and uncompressing 8TB of data!   But extraction of a few days or weeks of data will be relatively fast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce86ca9-07e5-4924-9ee6-b7ad0e476fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "import fsspec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4f007-e1ea-41fe-b6a7-83dcb2633bbb",
   "metadata": {},
   "source": [
    "#### Use Intake to load the consolidated NWM dataset\n",
    "The Intake catalog, the consolidated JSON file it accesses, and the NetCDF files the JSON file references are all on public S3 buckets that do not require an AWS account, so no credentials are required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93395642-f5dc-4a67-86be-ae6cfa469e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cat = intake.open_catalog('s3://esip-qhub-public/noaa/nwm/nwm_catalog.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacef894-4311-45cc-8d9c-2f4d5e8d8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a38c5e-2a08-420d-aacd-455050968f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = cat['nwm-reanalysis'].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b39cc-c485-49a4-a4bc-c99261ba119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "import ebdpy as ebd\n",
    "\n",
    "ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "profile = 'esip-qhub'\n",
    "region = 'us-west-2'\n",
    "endpoint = f's3.{region}.amazonaws.com'\n",
    "ebd.set_credentials(profile=profile, region=region, endpoint=endpoint)\n",
    "worker_max = 30\n",
    "client,cluster = ebd.start_dask_cluster(profile=profile,worker_max=worker_max, \n",
    "                                      region=region, use_existing_cluster=True,\n",
    "                                      adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                      environment='pangeo', worker_profile='Pangeo Worker', \n",
    "                                      propagate_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95d23d-13d9-4a7e-a2fd-4ff6e6846b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close(); cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656c93e-9c10-4676-a8c0-3e7f24306f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057a133-84dc-4d00-8ca8-9c3135bd197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rechunker import rechunk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966a01e-7c76-4fb9-96c6-3ba4368f5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chunk_size = 672   \n",
    "feature_chunk_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0638c-e776-4d15-b382-0b50aeca9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_chunks = len(ds.feature_id)/feature_chunk_size\n",
    "nh_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2cae4-7aaf-4bce-ae8b-12b9834bdf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_chunks = int(np.ceil(len(ds.time)/time_chunk_size))\n",
    "nt_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff70b63-0a73-403b-ac9c-8425ced304ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_s3(url):\n",
    "    fs1 = fsspec.open(url, anon=False).fs\n",
    "    if fs1.exists(url):\n",
    "        fs1.rm(url, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204099d-779e-40a2-9779-dc9bdf04d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_url = 's3://esip-qhub/usgs/zarr/nwm/chunked.zarr'\n",
    "step_url = 's3://esip-qhub/usgs/zarr/step/step.zarr'\n",
    "temp_url = 's3://esip-qhub/usgs/zarr/tmp/temp.zarr'\n",
    "\n",
    "fs2 = fsspec.filesystem('s3', anon=False, default_fill_cache=False, skip_instance_cache=True)\n",
    "\n",
    "delete_s3(chunked_url)\n",
    "zarr_chunked = fs2.get_mapper(chunked_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fc6fa-d19d-4e57-87b1-33cc9d1ed962",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs2.ls('s3://esip-qhub/usgs/zarr/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad89502-c060-4910-9f64-99fb3f62e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mem='1.8GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f9960-69e9-49ab-8452-f54127f044ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55b51a-88d9-4ea4-9ece-091ea4621b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks={'time':720, 'feature_id':30000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d59ee-c769-4228-8b91-c80edd45e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.streamflow.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014878c-f7d5-4d22-829d-b26a540511e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chunks = {}\n",
    "# newer tuple version that also takes into account when specified chunks are larger than the array\n",
    "for var in ds.variables:\n",
    "    # pick appropriate chunks from above, and default to full length chunks for dimensions that are not in `chunks` above.\n",
    "    group_chunks[var] = []\n",
    "    for di in ds[var].dims:\n",
    "        if di in chunks.keys():\n",
    "            if chunks[di] > len(ds[di]):\n",
    "                group_chunks[var].append(len(ds[di]))\n",
    "            else:\n",
    "                group_chunks[var].append(chunks[di])\n",
    "\n",
    "        else:\n",
    "            group_chunks[var].append(len([di]))\n",
    "\n",
    "    group_chunks[var] = tuple(group_chunks[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4e478-59c1-4740-afb6-ceea31ad0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811a3ce3-1fbb-484a-a991-69ff646e0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#for i in range(nt_chunks):\n",
    "for i in range(3):\n",
    "    print(i)\n",
    "    istart = i * time_chunk_size\n",
    "    istop = int(np.min([(i+1) * time_chunk_size, len(ds.time)]))\n",
    "    \n",
    "#    ds = xr.open_mfdataset(files[istart:istop], parallel=True, \n",
    "#                           preprocess=drop_coords, combine='by_coords', \n",
    "#                       concat_dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    ds2 = ds.isel(time=slice(istart,istop))\n",
    "\n",
    "    # remote the temp and step zarr datasets\n",
    "    # chunk this step to zarr using rechunker\n",
    "    delete_s3(step_url)\n",
    "    delete_s3(temp_url)\n",
    "    zarr_step = fs2.get_mapper(step_url)\n",
    "    zarr_temp = fs2.get_mapper(temp_url)\n",
    "    \n",
    "    for var in ds2.data_vars:\n",
    "        if len(ds2[var].dims)==2:\n",
    "\n",
    "            ds2[var].encoding['_FillValue'] = -999900\n",
    "            ds2[var].encoding['missing_value'] = -999900\n",
    "            ds2[var].encoding['dtype'] = 'int16'\n",
    "            ds2[var].encoding['chunks']: (720,30000)\n",
    "    array_plan = rechunk(ds2, group_chunks, max_mem, zarr_step, \n",
    "                     temp_store=zarr_temp)\n",
    "    \n",
    "    print('Executing rechunk for {}'.format(i))\n",
    "    with performance_report(filename=\"dask-report.html\"):\n",
    "        result = array_plan.execute(retries=10)\n",
    "\n",
    "        \n",
    "    print('Finished rechunk for {}'.format(i))\n",
    "    # read back in the zarr chunk rechunker wrote\n",
    "    ds3 = xr.open_zarr(zarr_step)\n",
    "\n",
    "    if i==0:\n",
    "        ds3.to_zarr(zarr_chunked, consolidated=True, mode='w')\n",
    "    else:\n",
    "        ds3.to_zarr(zarr_chunked, consolidated=True, append_dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81960998-f967-4c08-b2c9-905cf86dfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7235eaf-9b7e-49ff-b088-85800d234470",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2['streamflow'].encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108548d-8226-412c-a2ad-3c23466150d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
