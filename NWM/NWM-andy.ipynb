{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## National Water Model: Time series extraction\n",
    "Extract an hourly time series from one of the 2.7 million rivers in the 26-year NWM reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Inspired by [Rich Signell's notebook](https://nbviewer.jupyter.org/gist/rsignell-usgs/331eaa1fc9955d9c0e68f6b3ea8c22fb).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - Created by:   __Andy Carter, PE__\n",
    " >andy.carter@austin.utexas.edu\n",
    "\n",
    " - Created On:   __08 Jan 2021__<br><br>\n",
    " - Last revised:  __Verison 0.1__\n",
    " \n",
    " - Purpose:\n",
    " >From the NOAA 'ZARR' repository (on AWS) of the National Water Model v2.0 reanalysis, the goal is to slice a \n",
    " single NetCDF file representing flow for 1 hour timesteps from 1993-01-01 to the last recorded data.  This is about\n",
    " 26 years.  Attempting to write out one NetCDF file that has the hydrograph for a single COMID.\n",
    "\n",
    " - Inputs required:\n",
    "  > 1) COMID for the requested stream <br>\n",
    "    2) Path to write out the netCDF<br>\n",
    "    3) ESRI Shapefile of the area to be sampled (EPSG: zone does not matter)<br>\n",
    "    \n",
    " - Output generated:\n",
    "  > Single netCDF file of the flow every hour from the NWM v2.0 reanalysis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.0 References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import fsspec\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.0 Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intCOMID = 5790110\n",
    "strPathHeader = r\"C:\\NWM_Slice\\RiverFlow_\"\n",
    "strPathHeater = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'http://noaa-nwm-retro-v2.0-zarr-pds.s3.amazonaws.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternate if have read access to NOAA AWS s3 bucket... \n",
    "\n",
    "#url = 's3://esip-qhub/noaa/nwm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.0 Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_zarr(fsspec.get_mapper(url), consolidated=True)\n",
    "\n",
    "#Alternate with AWS S3 access credentials\n",
    "#ds = xr.open_zarr(fsspec.get_mapper(url, requester_pays=True), consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dm = ds.streamflow.sel(feature_id=intCOMID, time=slice('2014-09-01','2014-09-30')).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.hvplot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.to_series().to_csv('dm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 's3://noaa-nwm-retro-v2.0-zarr-pds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(fsspec.get_mapper(url, anon=True), consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dm = ds.streamflow.sel(feature_id=intCOMID).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "cluster = gateway.new_cluster(environment='pangeo', profile='Pangeo Worker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dm = ds.streamflow.sel(feature_id=intCOMID).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4.0 Save hydrograph data to netCDF on local drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strPath = strPathHeader + str(intCOMID) + \".nc\" \n",
    "dm.to_netcdf(path=strPath, mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = xr.open_dataset(strPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2['streamflow'].plot(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2['streamflow'].hvplot(grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 5.0 Notes\n",
    "\n",
    "-2021.01.08 - It appears that the request to the ZARR via the http request is not returning the full time series data.  Is this\n",
    "beacuse the local machine does not have aqequate memory for the xarray?  Would a container on an HPC with adequate memory be able to get these data.  Perhaps the ZARR via the HTTP is not complete (not seeing all chunks).  Currently unkown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
