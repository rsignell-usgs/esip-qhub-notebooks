{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert lots of small NetCDFs to one big Zarr\n",
    "The National Water Model writes a new NetCDF file for each hour, resulting in 8760 files for a year, and 227904 files for the entire 26 year reanalysis (1993-01-01 00:00 - 2018-12-31 23:00).  \n",
    "\n",
    "For small datasets, rechunking the data to Zarr would be as simple as:\n",
    "\n",
    "```\n",
    "import xarray as xr\n",
    "ds = xr.open_mfdataset('*.nc')\n",
    "ds = ds.chunk({'time':672, 'feature_id':30000})\n",
    "ds.to_zarr('all_nc.zarr', consolidated=True)\n",
    "```\n",
    "For large datasets, this approach is slow and uses too much memory.  Here we process the data in batches of 672 time files at a time (one time chunk).   \n",
    "\n",
    "For each batch, we create an xarray dataset with open_mfdataset, then use [rechunker](https://github.com/pangeo-data/rechunker), which creates a rechunked Zarr dataset for that batch.  We then append each batch (each time chunk) along the time dimension, building up our overall dataset.   \n",
    "\n",
    "The nice part of this approach is that if something goes wrong with the batch, we can fix the problem and just carry on appending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numcodecs\n",
    "from dask.distributed import Client, progress, LocalCluster, performance_report\n",
    "import zarr\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from rechunker import rechunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a list of filenames for open_mfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldir = '/home/shared/users/rsignell/data/NWM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start='1993-01-01 00:00',end='1993-12-31 23:00', freq='1h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date2file(date):\n",
    "    yyyymmddhh = date.strftime('%Y%m%d%H')\n",
    "    yyyy = date.strftime('%Y')\n",
    "    cfile = f's3://noaa-nwm-retro-v2.0-pds/full_physics/{yyyy}/{yyyymmddhh}00.LAKEOUT_DOMAIN1.comp'\n",
    "    fname = f'{ldir}/{yyyy}/{yyyymmddhh}.LAKEOUT_DOMAIN1.comp'\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [date2file(date) for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start='1993-01-01 00:00',end='2018-12-31 23:00', freq='1h')\n",
    "\n",
    "files = ['./nc/{}/{}.CHRTOUT_DOMAIN1.comp'.format(date.strftime('%Y'),date.strftime('%Y%m%d%H%M')) for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = xr.open_dataset(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice chunk size for object storage is on the order of 100Mb.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chunk_size = 672   \n",
    "feature_chunk_size = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_chunks = len(dset.feature_id)/feature_chunk_size\n",
    "nh_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_chunks = int(np.ceil(len(files)/time_chunk_size))\n",
    "nt_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(time_chunk_size * feature_chunk_size )*8 / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Close enough to 100Mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to drop stuff that messes up `open_mfdataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_coords(ds):\n",
    "    ds = ds.drop(['reference_time','feature_id', 'crs'])\n",
    "    return ds.reset_coords(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close(); client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a local dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell blosc not to use threads since we are using dask to parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcodecs.blosc.use_threads = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_step = f'{ldir}/zarr/step'\n",
    "zarr_chunked = f'{ldir}/zarr/nwm'\n",
    "zarr_temp = f'{ldir}/zarr/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step our way through the dataset, reading one chunk along the time dimension at a time, to avoid dask reading too many chunks before writing and blowing out memory.  First time chunk is written to zarr, then others are appended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Rechunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rechunker import rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mem='1.8GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(files[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(files[:200], concat_dim='time', \n",
    "combine='nested', coords='minimal', compat='override', engine='netcdf4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the big loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(nt_chunks):\n",
    "    print(i)\n",
    "    istart = i * time_chunk_size\n",
    "    istop = int(np.min([(i+1) * time_chunk_size, len(files)]))\n",
    "    \n",
    "#    ds = xr.open_mfdataset(files[istart:istop], parallel=True, \n",
    "#                           preprocess=drop_coords, combine='by_coords', \n",
    "#                       concat_dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    ds = xr.open_mfdataset(files[istart:istop],  combine='by_coords', \n",
    "                       concat_dim='time', preprocess=drop_coords, coords='minimal', compat='override')\n",
    "\n",
    "    # add back in the 'feature_id' coordinate removed by preprocessing \n",
    "    ds.coords['feature_id'] = dset.coords['feature_id']\n",
    "    # chunk this step to zarr using rechunker\n",
    "\n",
    "    # remote the temp and step zarr datasets\n",
    "    try:\n",
    "        shutil.rmtree(zarr_temp)\n",
    "        while os.path.exists(zarr_temp): # check if it still exists\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(zarr_step)\n",
    "        while os.path.exists(zarr_step): # check if it still exists\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    chunk_plan={}\n",
    "    for var in ds.data_vars:\n",
    "        if len(ds[var].dims)==2:\n",
    "            var_chunk = (time_chunk_size, feature_chunk_size)\n",
    "            chunk_plan[var] = var_chunk\n",
    "\n",
    "    array_plan = rechunk(ds, chunk_plan, max_mem, zarr_step, \n",
    "                     temp_store=zarr_temp)\n",
    "    \n",
    "    with performance_report(filename=\"dask-report.html\"):\n",
    "        result = array_plan.execute(retries=10)\n",
    "\n",
    "    # read back in the zarr chunk rechunker wrote\n",
    "    ds = xr.open_zarr(zarr_step)\n",
    "\n",
    "    if i==0:\n",
    "        ds.to_zarr(zarr_chunked, consolidated=True, mode='w')\n",
    "    else:\n",
    "        ds.to_zarr(zarr_chunked, consolidated=True, append_dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/home/shared/users/rsignell/data/NWM/1993/1993010821.LAKEOUT_DOMAIN1.comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.inflow.hvplot.scatter(x='longitude', y='latitude', color=ds.inflow.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.inflow.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow threw an error on the last partial chunk because Rechunker doesn't think the chunk_plan is valid.  But it is valid to have a partial last chunk.   Here we just rechunk the last partial chunk without rechunker and append it to the overall Zarr dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.chunk({'feature_id':feature_chunk_size, 'time':time_chunk_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.to_zarr('./zarr/last_step', consolidated=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = xr.open_zarr('/usgs/gamone/data2/rsignell/data/NWM2/zarr/last_step', consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.to_zarr(zarr_chunked, consolidated=True, append_dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the resulting chunked dataset for correct start time, stop time and for any gaps.  If there are no gaps we should get just a single unique value of 3600s for the difference between the hourly time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = xr.open_zarr('/usgs/gamone/data2/rsignell/data/NWM2/zarr/nwm', consolidated=True)\n",
    "print(ds1.time[0].values)\n",
    "print(ds1.time[-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = ds1.time.diff(dim='time').values/1e9   # convert datetime64 nanoseconds to seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close();  client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "ds1.streamflow[:,1000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
