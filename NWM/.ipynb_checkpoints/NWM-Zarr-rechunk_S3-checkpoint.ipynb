{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert lots of small NetCDFs to one big Zarr\n",
    "The National Water Model writes a new NetCDF file for each hour, resulting in 8760 files for a year, and 227904 files for the entire 26 year reanalysis (1993-01-01 00:00 - 2018-12-31 23:00).  \n",
    "\n",
    "For small datasets, rechunking the data to Zarr would be as simple as:\n",
    "\n",
    "```\n",
    "import xarray as xr\n",
    "ds = xr.open_mfdataset('*.nc')\n",
    "ds = ds.chunk({'time':672, 'feature_id':30000})\n",
    "ds.to_zarr('all_nc.zarr', consolidated=True)\n",
    "```\n",
    "For large datasets, this approach is slow and uses too much memory.  Here we process the data in batches of 672 time files at a time (one time chunk).   \n",
    "\n",
    "For each batch, we create an xarray dataset with open_mfdataset, then use [rechunker](https://github.com/pangeo-data/rechunker), which creates a rechunked Zarr dataset for that batch.  We then append each batch (each time chunk) along the time dimension, building up our overall dataset.   \n",
    "\n",
    "The nice part of this approach is that if something goes wrong with the batch, we can fix the problem and just carry on appending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numcodecs\n",
    "import dask\n",
    "from dask.distributed import Client, progress, performance_report\n",
    "import zarr\n",
    "import time\n",
    "import fsspec\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fsspec.__version__)\n",
    "print(s3fs.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates = pd.date_range(start='1993-01-01 00:00',end='2018-12-31 23:00', freq='1h')\n",
    "dates = pd.date_range(start='2018-01-01 00:00',end='2018-01-31 23:00', freq='1h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a list of filenames for open_mfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def s3open(path):\n",
    "    fs = fsspec.filesystem('s3', anon=True, default_fill_cache=False)\n",
    "    return fs.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = 's3://noaa-nwm-retro-v2.0-pds/full_physics/{}/{}.CHRTOUT_DOMAIN1.comp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True, default_fill_cache=False)\n",
    "first_file = s3path.format(dates[0].strftime('%Y'),dates[0].strftime('%Y%m%d%H%M'))\n",
    "ncfile = fs.open(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = xr.open_dataset(ncfile, engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "files = [s3open(s3path.format(date.strftime('%Y'),date.strftime('%Y%m%d%H%M'))) for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice chunk size for object storage is on the order of 100Mb.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chunk_size = 288\n",
    "feature_chunk_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_chunks = len(dset.feature_id)/feature_chunk_size\n",
    "nh_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_chunks = int(np.ceil(len(files)/time_chunk_size))\n",
    "nt_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(time_chunk_size * feature_chunk_size )*8 / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Close enough to 100Mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to drop stuff that messes up `open_mfdataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_coords(ds):\n",
    "    ds = ds.drop(['reference_time','feature_id', 'crs'])\n",
    "    return ds.reset_coords(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close(); client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gateway.stop_cluster(cluster_name='932673464290474fb39b0b518655ac63')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "import ebdpy as ebd\n",
    "\n",
    "?ebd.start_dask_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = 'esip-qhub'\n",
    "region = 'us-west-2'\n",
    "endpoint = f's3.{region}.amazonaws.com'\n",
    "ebd.set_credentials(profile=profile, region=region, endpoint=endpoint)\n",
    "worker_max = 20\n",
    "client,cluster = ebd.start_dask_cluster(profile=profile,worker_max=worker_max, \n",
    "                                      region=region, use_existing_cluster=False,\n",
    "                                      adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                      environment='default', worker_profile='Pangeo Worker', propagate_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell blosc not to use threads since we are using dask to parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcodecs.blosc.use_threads = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_s3(url):\n",
    "    fs = fsspec.open(url, anon=False).fs\n",
    "    if fs.exists(url):\n",
    "        fs.rm(url, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_url = 's3://esip-qhub/usgs/zarr/nwm/chunked.zarr'\n",
    "step_url = 's3://esip-qhub/usgs/zarr/step/step.zarr'\n",
    "temp_url = 's3://esip-qhub/usgs/zarr/tmp/temp.zarr'\n",
    "\n",
    "fs = fsspec.filesystem('s3', anon=False)\n",
    "\n",
    "zarr_chunked = fs.get_mapper(chunked_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close; cluster.shutdown()\n",
    "#client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.nbytes/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.run(os.path.expanduser, \"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this is you have already started your zarr and are happy with it!!!\n",
    "delete_s3(chunked_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#dset.to_zarr(zarr_chunked, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.retire_workers([k for k, v in client.processing().items() if v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls('esip-qhub/usgs/zarr/step/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = xr.open_zarr(zarr_chunked, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs.ls(chunked_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step our way through the dataset, reading one chunk along the time dimension at a time, to avoid dask reading too many chunks before writing and blowing out memory.  First time chunk is written to zarr, then others are appended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Rechunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rechunker import rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_mem_factor = 0.8  #fraction of worker memory for each chunk (seems to be the max possible)\n",
    "worker_mem = 5e9/1e9 #cluster.worker_spec[0]['options']['memory_limit']/1e9\n",
    "max_mem = '{:.2f}GB'.format(chunk_mem_factor * worker_mem)\n",
    "print(max_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mem = '1.6GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_s3(step_url)\n",
    "delete_s3(temp_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the big loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### %%time\n",
    "for i in range(nt_chunks):\n",
    "    print(i)\n",
    "    istart = i * time_chunk_size\n",
    "    istop = int(np.min([(i+1) * time_chunk_size, len(files)]))\n",
    "    \n",
    "#    ds = xr.open_mfdataset(files[istart:istop], parallel=True, preprocess=drop_coords, combine='by_coords', \n",
    "#                       concat_dim='time', join='override', engine='h5netcdf', backend_kwargs={'decode_vlen_strings':True})\n",
    "\n",
    "    ds = xr.open_mfdataset(files[istart:istop], parallel=True, preprocess=drop_coords, combine='by_coords', \n",
    "                       concat_dim='time', join='override', engine='h5netcdf')\n",
    "    \n",
    "    print('Finished opening for {}'.format(i))\n",
    "    # add back in the 'feature_id' coordinate removed by preprocessing \n",
    "    ds.coords['feature_id'] = dset.coords['feature_id']\n",
    "    \n",
    "    # chunk this step to zarr using rechunker\n",
    "    delete_s3(step_url)\n",
    "    delete_s3(temp_url)\n",
    "    zarr_step = fsspec.get_mapper(step_url, anon=False)\n",
    "    zarr_temp = fsspec.get_mapper(temp_url, anon=False)\n",
    "    \n",
    "    chunk_plan={}\n",
    "    for var in ds.data_vars:\n",
    "        if len(ds[var].dims)==2:\n",
    "            var_chunk = (time_chunk_size, feature_chunk_size)\n",
    "            chunk_plan[var] = var_chunk\n",
    "\n",
    "    array_plan = rechunk(ds, chunk_plan, max_mem, zarr_step, \n",
    "                 temp_store=zarr_temp)\n",
    "    \n",
    "    \n",
    "    print('Executing rechunk for {}'.format(i))\n",
    "    with performance_report(filename=\"dask-report.html\"):\n",
    "        result = array_plan.execute(retries=10)\n",
    "\n",
    "        \n",
    "    print('Finished rechunk for {}'.format(i))\n",
    "    # read back in the zarr chunk rechunker wrote\n",
    "    ds = xr.open_zarr(zarr_step)\n",
    "\n",
    "    if i==0:\n",
    "#        compressor = zarr.Blosc(cname='zstd', clevel=5, shuffle=zarr.Blosc.AUTOSHUFFLE)\n",
    "#        encoding = {v: {'compressor': compressor, '_FillValue': -9999.0 } for v in ds.data_vars}\n",
    "#        ds.to_zarr(zarr_chunked, consolidated=True, mode='w', encoding=encoding)\n",
    "        ds.to_zarr(zarr_chunked, consolidated=True, mode='w')\n",
    "    else:\n",
    "        ds.to_zarr(zarr_chunked, consolidated=True, append_dim='time')\n",
    "    \n",
    "    print('Finished writing for {}'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = xr.open_zarr(zarr_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow threw an error on the last partial chunk because Rechunker doesn't think the chunk_plan is valid.  But it is valid to have a partial last chunk.   Here we just rechunk the last partial chunk without rechunker and append it to the overall Zarr dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds2['streamflow'][:,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.to_zarr(zarr_chunked, consolidated=True, append_dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the resulting chunked dataset for correct start time, stop time and for any gaps.  If there are no gaps we should get just a single unique value of 3600s for the difference between the hourly time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = xr.open_zarr('/usgs/gamone/data2/rsignell/data/NWM2/zarr/nwm', consolidated=True)\n",
    "print(ds1.time[0].values)\n",
    "print(ds1.time[-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = ds1.time.diff(dim='time').values/1e9   # convert datetime64 nanoseconds to seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close();  client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "ds1.streamflow[:,1000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:default]",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
