{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## National Water Model: Time series extraction\n",
    "Extract an hourly time series from one of the 2.7 million rivers in the 26-year NWM reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Inspired by [Rich Signell's notebook](https://nbviewer.jupyter.org/gist/rsignell-usgs/331eaa1fc9955d9c0e68f6b3ea8c22fb).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - Created by:   __Andy Carter, PE__\n",
    " >andy.carter@austin.utexas.edu\n",
    "\n",
    " - Created On:   __08 Jan 2021__<br><br>\n",
    " - Last revised:  __Verison 0.1__\n",
    " \n",
    " - Purpose:\n",
    " >From the NOAA 'ZARR' repository (on AWS) of the National Water Model v2.0 reanalysis, the goal is to slice a \n",
    " single NetCDF file representing flow for 1 hour timesteps from 1993-01-01 to the last recorded data.  This is about\n",
    " 26 years.  Attempting to write out one NetCDF file that has the hydrograph for a single COMID.\n",
    "\n",
    " - Inputs required:\n",
    "  > 1) COMID for the requested stream <br>\n",
    "    2) Path to write out the netCDF<br>\n",
    "    3) ESRI Shapefile of the area to be sampled (EPSG: zone does not matter)<br>\n",
    "    \n",
    " - Output generated:\n",
    "  > Single netCDF file of the flow every hour from the NWM v2.0 reanalysis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.0 References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import fsspec\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.0 Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intCOMID = 5790110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 's3://noaa-nwm-retro-v2.0-zarr-pds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.0 Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_zarr(fsspec.get_mapper(url), consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dm = ds.streamflow.sel(feature_id=intCOMID, time=slice('2014-09-01','2014-09-30')).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.hvplot(grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4.0 Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.to_series().to_csv('dm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
