{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STWAVE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fsspec\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Coastal Coupling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', requester_pays=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls('esip-qhub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls('esip-qhub/usace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data using the cloud-friendly zarr data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = fs.get_mapper('s3://esip-qhub/usace/USACE-2016-stwave-regional.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(store, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = ds.where(ds!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['waveHs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many GB of wave height data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['waveHs'].nbytes/1.e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a dask cluster to crunch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "gateway = Gateway()\n",
    "gateway.list_clusters()\n",
    "if gateway.list_clusters():\n",
    "    print('Existing Dask clusters:')\n",
    "    for c in gateway.list_clusters():\n",
    "        print('Cluster Name:',c.name,c.status)\n",
    "else:\n",
    "    print('No Cluster running.')\n",
    "# New or connect:\n",
    "# If no cluster is running, create a new one, else connect to the first one found (idx=0, change if other cluster should be running)\n",
    "idx=0\n",
    "if not gateway.list_clusters():\n",
    "    cluster = gateway.new_cluster(environment='pangeo', profile='Small Worker')\n",
    "else:\n",
    "    cluster=gateway.connect(gateway.list_clusters()[idx].name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway.cluster_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask_kubernetes import KubeCluster\n",
    "#cluster = KubeCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize using HoloViz.org tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['waveHs'][:,40,40].hvplot(x='time', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['waveHs'].sel(time=slice('2016-11-01 00:00','2016-11-30 23:00'))[:,40,40].hvplot(x='time', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ds['waveHs'].sel(time='2016-11-08 16:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.where(da!=0).hvplot.quadmesh(x='longitude', y='latitude',\n",
    "                    colormap='rainbow', rasterize=True, geo=True, \n",
    "                    tiles='OSM', clim=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the maximum over the time dimension and persist the data on the workers to use later.  This is the computationally intensive step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "max_var = ds['waveHs'].max(dim='time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_var.where(max_var!=0).hvplot.quadmesh(x='longitude', y='latitude',\n",
    "                    colormap='rainbow', rasterize=True, geo=True,\n",
    "                    tiles='OSM', clim=(3,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a time series at a lon,lat location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find nearest_point for a requested lat/ lon\n",
    "def findij(lon2d, lat2d, lon_sta, lat_sta):\n",
    "    d_lat = lat2d - lat_sta\n",
    "    d_lon = lon2d - lon_sta\n",
    "    r2_requested = d_lat**2 + d_lon**2\n",
    "    (ii,jj) = np.where(r2_requested == np.min(r2_requested))\n",
    "    return (ii,jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_sta = 36.2\n",
    "lon_sta = -75.65\n",
    "[jj,ii] = findij(ds.longitude, ds.latitude, lon_sta, lat_sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['waveHs'][:,jj,ii].hvplot(x='time', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_sta = 36.2\n",
    "lon_sta = -75.73\n",
    "[jj,ii] = findij(ds.longitude, ds.latitude, lon_sta, lat_sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['waveHs'][:,jj,ii].hvplot(x='time', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When done, you can do \"File=>Close and Shutdown Notebook\" from the menu, shutdown the client and cluster thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close();cluster.shutdown();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or scale to 1 without killing the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.scale(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
